{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"conve_single_cpu_and_gpu_2layer.ipynb","version":"0.3.2","provenance":[{"file_id":"1Gdny5cRRTAfeLwEDzR48a-DqTviM25fe","timestamp":1544178941888}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"D7PbAsPQJajk","colab_type":"text"},"cell_type":"markdown","source":["1. Run this file: On Single CPU, GPU, and Turing cluster using command \"python filename\".\n","2. Saved the model_dir and output testfor each run after training and testing\n","3. Change parameters model_dir for cpu/gpu and use both DB100K and FB15K for now.\n","4. Output will be three model_dir and output text one for each instances CPU, GPU, turing."]},{"metadata":{"id":"Zh5bYbYMJVj5","colab_type":"code","outputId":"13a2a2bd-ece2-442a-bca2-2bddae7a57e9","executionInfo":{"status":"ok","timestamp":1544351796107,"user_tz":-330,"elapsed":6275,"user":{"displayName":"Nilesh Agrawal","photoUrl":"","userId":"15528777783423233741"}},"colab":{"base_uri":"https://localhost:8080/","height":156}},"cell_type":"code","source":["from tqdm import tqdm\n","import tensorflow as tf\n","\n","import numpy as np\n","import sklearn\n","import pprint\n","import itertools\n","import os\n","import time\n","import sys\n","import zipfile\n","!pip install wget\n","import wget\n","\n","sys.path.append(os.path.dirname(os.getcwd()))\n","tf.logging.set_verbosity(tf.logging.INFO)\n","# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting wget\n","  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n","Building wheels for collected packages: wget\n","  Running setup.py bdist_wheel for wget ... \u001b[?25l-\b \bdone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n","Successfully built wget\n","Installing collected packages: wget\n","Successfully installed wget-3.2\n"],"name":"stdout"}]},{"metadata":{"id":"qsG1vbw2Xwhv","colab_type":"code","colab":{}},"cell_type":"code","source":["class Config:\n","    n_epochs = 100\n","    batch_size = 128\n","    embed_dim = 200\n","    prefetch_buffer_size = None\n","    shuffle_buffer_size = 2\n","    map_threads = 3\n","    data_path = \"data/\"       #constant value\n","    dataset_name=\"DB100K\"      #FB15K/WN18/DB100K     #FB15K 272115, maxlen = 843         # DB100K 597572, maxlen = 85\n","    model_dir = \"model/conve_\"+\"single_gpu\"+\"_bs_\"+str(batch_size)+\"_epochs_\"+str(n_epochs)+\"_embed_dim_\"+str(embed_dim)+\"_dataset_\"+dataset_name # single_cpu / single_gpu"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IDKy5bwxXdy7","colab_type":"code","colab":{}},"cell_type":"code","source":["def download(data_path=Config.data_path, dataset_name=Config.dataset_name):\n","    dataset_path=data_path+dataset_name\n","    if not os.path.exists(dataset_path):\n","      os.makedirs(dataset_path)\n","    dataset_path = dataset_path+'/'\n","    if dataset_name==\"WN18\":\n","      if not os.path.isfile(dataset_path+'train.txt'):\n","          zip_path = dataset_path+\"WN18RR.zip\"\n","          url = \"https://www.dropbox.com/s/sginaquks2xzv6o/WN18RR.zip?dl=1\"\n","          wget.download(url, zip_path)\n","          z = zipfile.ZipFile(zip_path, 'r')\n","          z.extractall(dataset_path)\n","          z = z.close()\n","          os.remove(zip_path)\n","      else:\n","          print('Files Already Downloaded')\n","    elif dataset_name==\"FB15K\":\n","      if not os.path.isfile(dataset_path+'train.txt'):\n","        zip_path = dataset_path+\"FB15K.zip\"\n","        url = \"https://www.dropbox.com/s/kph0mbs79w8itw6/FB15K-237.zip?dl=1\"\n","        wget.download(url, zip_path)\n","        z = zipfile.ZipFile(zip_path, 'r')\n","        z.extractall(dataset_path)\n","        z = z.close()\n","        os.remove(zip_path)\n","      else:\n","          print('Files Already Downloaded')\n","    elif dataset_name==\"DB100K\":\n","      if not os.path.isfile(dataset_path+'train.txt'):\n","        zip_path = dataset_path+\"DB100K.zip\"\n","        url = \"https://www.dropbox.com/s/fmbbh712ilx2zrc/DB100K.zip?dl=1\"\n","        wget.download(url, zip_path)\n","        z = zipfile.ZipFile(zip_path, 'r')\n","        z.extractall(dataset_path)\n","        z = z.close()\n","        os.remove(zip_path)\n","      else:\n","          print('Files Already Downloaded')\n","download()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"g7m3a1svXdzB","colab_type":"code","colab":{}},"cell_type":"code","source":["\"\"\"\n","e: entity\n","s: subject\n","p: predicate\n","o: object\n","\"\"\"\n","def glance_dict(d, n=5):\n","    return dict(itertools.islice(d.items(), n))\n","\n","def read_triples(path):\n","    triples = []\n","    with open(path, 'rt') as f:\n","        for line in f.readlines():\n","            s, p, o = line.split()\n","            triples += [(s.strip(), p.strip(), o.strip())]\n","    return triples\n","\n","def load_triple():\n","    download()\n","    dataset_path = Config.data_path+Config.dataset_name+\"/\"\n","    triples_tr = read_triples(dataset_path+'train.txt')\n","    triples_va = read_triples(dataset_path+'valid.txt')\n","    triples_te = read_triples(dataset_path+'test.txt')\n","    triples_all = triples_tr + triples_va + triples_te\n","    return triples_all, triples_tr, triples_va, triples_te\n","\n","\n","def build_vocab(triples):\n","    params = {}\n","    e_set = {s for (s, p, o) in triples} | {o for (s, p, o) in triples}\n","    p_set = {p for (s, p, o) in triples}\n","    params['e_vocab_size'] = len(e_set)\n","    params['p_vocab_size'] = len(p_set)\n","    e2idx = {e: idx for idx, e in enumerate(sorted(e_set))}\n","    p2idx = {p: idx for idx, p in enumerate(sorted(p_set))}\n","    return e2idx, p2idx, params\n","\n","\n","def build_multi_label(triples_tr):\n","    sp2o = {}\n","    for (_s, _p, _o) in triples_tr:\n","        s, p, o = e2idx[_s], p2idx[_p], e2idx[_o] \n","        if (s,p) not in sp2o:\n","            sp2o[(s,p)] = [o]\n","        else:\n","            if o not in sp2o[(s,p)]:\n","                sp2o[(s,p)].append(o)\n","    return sp2o\n","  \n","def build_multi_label_all(triples_all):\n","    sp2o_all = {}\n","    for (_s, _p, _o) in triples_all:\n","        s, p, o = e2idx[_s], p2idx[_p], e2idx[_o] \n","        if (s,p) not in sp2o_all:\n","            sp2o_all[(s,p)] = [o]\n","        else:\n","            if o not in sp2o_all[(s,p)]:\n","                sp2o_all[(s,p)].append(o)\n","    return sp2o_all  \n","\n","\n","def get_train_y(triples_tr, e2idx, p2idx, sp2o):\n","    y = []\n","    for (_s, _p, _o) in triples_tr:\n","        s, p, o = e2idx[_s], p2idx[_p], e2idx[_o] \n","        temp = np.zeros([len(e2idx)])\n","        temp[sp2o[(s,p)]] = 1.\n","        y.append(temp)\n","    y = np.asarray(y)\n","    return y\n","\n","def get_eval_y(triples_te, e2idx, p2idx, sp2o_all):\n","    y = []\n","    for (_s, _p, _o) in triples_te:\n","        s, p, o = e2idx[_s], p2idx[_p], e2idx[_o] \n","        temp1 = np.zeros([len(e2idx)])\n","        temp1[o] = 1.\n","        temp2 = np.ones([len(e2idx)])\n","        temp2[sp2o_all[(s,p)]] = -1.\n","        temp2[o] = 1.\n","        y.append((temp1,temp2))\n","    y = np.asarray(y)\n","    return y \n","  \n","def get_features_labels(triples_tr, e2idx, p2idx, sp2o,entity_vocab_size):\n","  features = [[e2idx[s],p2idx[p]] for (s, p, o) in triples_tr]\n","  labels = [sp2o[(e2idx[s],p2idx[p])] for (s, p, o) in triples_tr]\n","  new_labels = np.full([len(labels),len(max(labels,key = lambda x: len(x)))],params['e_vocab_size']+1)\n","  for i,j in enumerate(labels):\n","    new_labels[i][0:len(j)] = j\n","  return features, new_labels"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GejH61frLgnT","colab_type":"code","colab":{}},"cell_type":"code","source":["def next_train_batch(triples_tr, e2idx, p2idx, sp2o):\n","    for i in range(0, len(triples_tr), Config.batch_size):\n","        _triples_tr = triples_tr[i: i+Config.batch_size]\n","        x_s = np.asarray([e2idx[s] for (s, p, o) in _triples_tr], dtype=np.int32)\n","        x_p = np.asarray([p2idx[p] for (s, p, o) in _triples_tr], dtype=np.int32)\n","        y = get_train_y(_triples_tr, e2idx, p2idx, sp2o)\n","        yield ({'s': x_s, 'p': x_p}, y)\n","\n","def train_input_fn(triples_tr, e2idx, p2idx, sp2o):\n","    dataset = tf.data.Dataset.from_generator(\n","        lambda: next_train_batch(triples_tr,\n","                                 e2idx,\n","                                 p2idx,\n","                                 sp2o),\n","        ({'s': tf.int32, 'p': tf.int32}, tf.float32),\n","        ({'s': tf.TensorShape([None]), 'p': tf.TensorShape([None])},\n","         tf.TensorShape([None, len(e2idx)])))\n","    #dataset = dataset.shard(num_shards= ,  index = )\n","    dataset = dataset.shuffle(buffer_size=Config.shuffle_buffer_size)\n","    dataset = dataset.repeat(Config.n_epochs)\n","    #dataset = dataset.batch(Config.batch_size)\n","    dataset = dataset.prefetch(Config.prefetch_buffer_size)\n","    return dataset\n","  \n","def next_train_single(triples_tr, e2idx, p2idx, sp2o):\n","    for i in range(0, len(triples_tr)):\n","        s,p,o = triples_tr[i]\n","        x_s = e2idx[s]\n","        x_p = p2idx[p] \n","        y = np.zeros([len(e2idx)])\n","        y[sp2o[(x_s,x_p)]] = 1.\n","        yield ({'s': x_s, 'p': x_p}, y)\n","        \n","def train_input_fn_single(triples_tr, e2idx, p2idx, sp2o):\n","    dataset = tf.data.Dataset.from_generator(\n","        lambda: next_train_single(triples_tr,\n","                                 e2idx,\n","                                 p2idx,\n","                                 sp2o),\n","        ({'s': tf.int32, 'p': tf.int32}, tf.float32),\n","        ({'s': tf.TensorShape([]), 'p': tf.TensorShape([])},\n","         tf.TensorShape([len(e2idx)])))\n","    #dataset = dataset.shard(num_shards= ,  index = )\n","    dataset = dataset.shuffle(buffer_size=Config.shuffle_buffer_size)\n","    dataset = dataset.repeat(Config.n_epochs)\n","    dataset = dataset.batch(Config.batch_size)\n","    dataset = dataset.prefetch(Config.prefetch_buffer_size)\n","    return dataset\n","\n","def next_eval_batch(triples_te, e2idx, p2idx, sp2o_all):\n","    for i in range(0, len(triples_te), Config.batch_size):\n","        _triples_te = triples_te[i: i+Config.batch_size]\n","        x_s = np.asarray([e2idx[s] for (s, p, o) in _triples_te], dtype=np.int32)\n","        x_p = np.asarray([p2idx[p] for (s, p, o) in _triples_te], dtype=np.int32)\n","        y = get_eval_y(_triples_te, e2idx, p2idx, sp2o_all)\n","        yield ({'s': x_s, 'p': x_p}, y)\n","  \n","def eval_input_fn(triples_te, e2idx, p2idx, sp2o_all):\n","    dataset = tf.data.Dataset.from_generator(\n","        lambda: next_eval_batch(triples_te,\n","                             e2idx, \n","                             p2idx,\n","                             sp2o_all),\n","        ({'s': tf.int32, 'p': tf.int32}, tf.float32),\n","        ({'s': tf.TensorShape([None]), 'p': tf.TensorShape([None])},\n","         tf.TensorShape([None, 2, len(e2idx)])))\n","    return dataset.prefetch(1)\n","  \n","def next_predict_batch(triples, e2idx, p2idx):\n","    for i in range(0, len(triples), Config.batch_size):\n","        _triples = triples[i: i+Config.batch_size]\n","        x_s = np.asarray([e2idx[s] for (s, p, o) in _triples], dtype=np.int32)\n","        x_p = np.asarray([p2idx[p] for (s, p, o) in _triples], dtype=np.int32)\n","        yield {'s': x_s, 'p': x_p}\n","  \n","def predict_input_fn(triples,\n","               e2idx, \n","               p2idx):\n","    dataset = tf.data.Dataset.from_generator(\n","        lambda: next_predict_batch(triples,\n","                             e2idx, \n","                             p2idx),\n","        ({'s':tf.int32, 'p':tf.int32}),\n","        ({'s':tf.TensorShape([None]),\n","         'p':tf.TensorShape([None])}))\n","    return dataset.prefetch(1)  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"0JmmEkJfXdzP","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","def tf_get_rank_order(input, targets, filtered):\n","      target1 = targets[:,0]\n","      target2 = targets[:,1]\n","      tf.logging.info\n","      size = tf.shape(input)[-1]\n","      if filtered:\n","        filtered_input = tf.multiply(input,target2)\n","        indices_of_ranks = tf.nn.top_k(-filtered_input, k=size)[1]\n","      else:\n","        indices_of_ranks = tf.nn.top_k(-input, k=size)[1]\n","      indices_of_ranks = size - tf.nn.top_k(-indices_of_ranks, k=size)[1]\n","      indices_of_o = tf.cast(tf.argmax(target1, axis=1),tf.int32)\n","      row_indices = tf.range(tf.shape(indices_of_o)[0])\n","      full_indices = tf.stack([row_indices, indices_of_o], axis=1)\n","      return tf.gather_nd(indices_of_ranks, full_indices)\n","\n","def get_rank(logits, targets, filtered = False):\n","    ordered_array = tf_get_rank_order(logits, targets, filtered)\n","    return ordered_array"],"execution_count":0,"outputs":[]},{"metadata":{"id":"960UHskCXdzU","colab_type":"code","colab":{}},"cell_type":"code","source":["def forward(features, mode, params):\n","    batch_sz = tf.shape(features['s'])[0]\n","    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n","    \n","    e_embed = tf.get_variable('e_embed',\n","                              [params['e_vocab_size'], Config.embed_dim],\n","                              initializer=tf.variance_scaling_initializer())\n","    p_embed = tf.get_variable('p_embed',\n","                              [params['p_vocab_size'], Config.embed_dim],\n","                              initializer=tf.variance_scaling_initializer())\n","    \n","    s = tf.nn.embedding_lookup(e_embed, features['s'])\n","    p = tf.nn.embedding_lookup(p_embed, features['p'])\n","    \n","    x = tf.concat([tf.reshape(s, [batch_sz, 10, 20, 1]),\n","                   tf.reshape(p, [batch_sz, 10, 20, 1]),], axis=1)\n","    x = tf.layers.batch_normalization(x, training=is_training)\n","    x = tf.layers.dropout(x, 0.2, training=is_training)\n","    \n","    x = tf.layers.conv2d(x, 64, (3,3), activation=tf.nn.relu)\n","    x = tf.layers.batch_normalization(x, training=is_training)\n","    x = tf.layers.dropout(x, 0.2, training=is_training)\n","\n","    x = tf.layers.conv2d(x, 32, (3,3), activation=tf.nn.relu)\n","    x = tf.layers.flatten(x)\n","    x = tf.layers.batch_normalization(x, training=is_training)\n","    x = tf.layers.dropout(x, 0.2, training=is_training)\n","    \n","    \n","    x = tf.layers.dense(x, Config.embed_dim, tf.nn.relu)\n","    x = tf.layers.batch_normalization(x, training=is_training)\n","    x = tf.layers.dropout(x, 0.3, training=is_training)\n","    \n","    logits = tf.matmul(x, e_embed, transpose_b=True)\n","    return logits\n","    \n","    \n","def model_fn(features, labels, mode, params):\n","    print(\"Shape of features: {} and labels: {}\".format(features['s'].shape,labels.shape))\n","    logits = forward(features, mode, params)\n","    \n","    if mode == tf.estimator.ModeKeys.TRAIN:\n","        tf.logging.info('\\n'+pprint.pformat(tf.trainable_variables()))\n","        tf.logging.info('params: %d'%count_train_params())\n","        \n","        global_step = tf.train.get_global_step()\n","        \n","        loss_op = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,\n","                                                                        labels=labels))\n","        \n","        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n","            train_op = tf.train.AdamOptimizer().minimize(loss_op,\n","                                                         global_step = global_step)\n","        \n","        return tf.estimator.EstimatorSpec(mode = mode,\n","                                          loss = loss_op,\n","                                          train_op = train_op)\n","    \n","    if mode == tf.estimator.ModeKeys.PREDICT:\n","        return tf.estimator.EstimatorSpec(mode = mode, predictions = tf.sigmoid(logits))\n","      \n","    if mode == tf.estimator.ModeKeys.EVAL:\n","        filtered_rank = tf.cast(get_rank(tf.sigmoid(logits), labels, filtered = True), tf.float32)\n","        raw_rank = tf.cast(get_rank(tf.sigmoid(logits), labels, filtered = False), tf.float32)\n","        metrics = {\n","          'mean_rank_raw': tf.metrics.mean(raw_rank),\n","          'mean_rank_filtered': tf.metrics.mean(filtered_rank),\n","          'mean_reciprocal_rank_raw': tf.metrics.mean(1./raw_rank),\n","          'mean_reciprocal_rank_filtered': tf.metrics.mean(1./filtered_rank),\n","          'hits_at_1_raw' : tf.metrics.mean(raw_rank <= 1.),\n","          'hits_at_3_raw' : tf.metrics.mean(raw_rank <= 3.),\n","          'hits_at_5_raw' : tf.metrics.mean(raw_rank <= 5.),\n","          'hits_at_10_raw' : tf.metrics.mean(raw_rank <= 10.),\n","          'filtered_at_1_raw' : tf.metrics.mean(filtered_rank <= 1.),\n","          'filtered_at_3_raw' : tf.metrics.mean(filtered_rank <= 3.),\n","          'filtered_at_5_raw' : tf.metrics.mean(filtered_rank <= 5.),\n","          'filtered_at_10_raw' : tf.metrics.mean(filtered_rank <= 10.)\n","        }\n","        loss_op = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,\n","                                                                        labels=labels[:,0]))\n","        return tf.estimator.EstimatorSpec(mode = mode,loss=loss_op, eval_metric_ops=metrics)\n","\n","def count_train_params():\n","    return np.sum([np.prod([d.value for d in v.get_shape()]) for v in tf.trainable_variables()])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OqSsh25dMsY5","colab_type":"code","outputId":"a1af95da-9391-417c-8f28-b6baee840775","executionInfo":{"status":"ok","timestamp":1544352074552,"user_tz":-330,"elapsed":6559,"user":{"displayName":"Nilesh Agrawal","photoUrl":"","userId":"15528777783423233741"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["triples_all, triples_tr, triples_va, triples_te = load_triple()\n","e2idx, p2idx, params = build_vocab(triples_all)\n","sp2o = build_multi_label(triples_tr)\n","sp2o_all = build_multi_label_all(triples_all)\n","# features, labels = get_features_labels(triples_tr, e2idx, p2idx, sp2o,params['e_vocab_size'] )"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Files Already Downloaded\n"],"name":"stdout"}]},{"metadata":{"id":"p4G7_Aqah7dK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"2c23e1d5-c987-4519-d481-2bee49340b99","executionInfo":{"status":"ok","timestamp":1544352076266,"user_tz":-330,"elapsed":761,"user":{"displayName":"Nilesh Agrawal","photoUrl":"","userId":"15528777783423233741"}}},"cell_type":"code","source":["print (len(triples_tr), len(triples_va), len(triples_te))\n","print(len(e2idx), len(p2idx))"],"execution_count":20,"outputs":[{"output_type":"stream","text":["597572 50000 50000\n","99604 470\n"],"name":"stdout"}]},{"metadata":{"id":"vV8eRVy-M07E","colab_type":"code","outputId":"cd76c44d-936f-419a-9c7c-744aec34e4da","executionInfo":{"status":"ok","timestamp":1544179296411,"user_tz":-330,"elapsed":62218,"user":{"displayName":"Nilesh Agrawal","photoUrl":"","userId":"15528777783423233741"}},"colab":{"base_uri":"https://localhost:8080/","height":1426}},"cell_type":"code","source":["config = tf.estimator.RunConfig(model_dir=Config.model_dir)\n","model = tf.estimator.Estimator(model_fn,\n","                               params = params,\n","                              config = config)\n","training_time = time.time()\n","tf.logging.info('Model Training Started at Time: {}'.format(time.time()))\n","model.train(lambda: train_input_fn(triples_tr, e2idx, p2idx, sp2o))\n","tf.logging.info('Model Training Ended at Time: {}'.format(time.time()))\n","training_time = time.time() - training_time "],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Using config: {'_model_dir': 'model/conve_single_cpu_bs_128_epochs_1_embed_dim_200_dataset_FB15K', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f7830c73400>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n","INFO:tensorflow:Model Training Started at Time: 1544179235.7107458\n","INFO:tensorflow:Calling model_fn.\n","Shape of features: (?,) and labels: (?, 14541)\n","INFO:tensorflow:\n","[<tf.Variable 'e_embed:0' shape=(14541, 200) dtype=float32_ref>,\n"," <tf.Variable 'p_embed:0' shape=(237, 200) dtype=float32_ref>,\n"," <tf.Variable 'batch_normalization/gamma:0' shape=(1,) dtype=float32_ref>,\n"," <tf.Variable 'batch_normalization/beta:0' shape=(1,) dtype=float32_ref>,\n"," <tf.Variable 'conv2d/kernel:0' shape=(3, 3, 1, 64) dtype=float32_ref>,\n"," <tf.Variable 'conv2d/bias:0' shape=(64,) dtype=float32_ref>,\n"," <tf.Variable 'batch_normalization_1/gamma:0' shape=(64,) dtype=float32_ref>,\n"," <tf.Variable 'batch_normalization_1/beta:0' shape=(64,) dtype=float32_ref>,\n"," <tf.Variable 'conv2d_1/kernel:0' shape=(3, 3, 64, 32) dtype=float32_ref>,\n"," <tf.Variable 'conv2d_1/bias:0' shape=(32,) dtype=float32_ref>,\n"," <tf.Variable 'batch_normalization_2/gamma:0' shape=(8192,) dtype=float32_ref>,\n"," <tf.Variable 'batch_normalization_2/beta:0' shape=(8192,) dtype=float32_ref>,\n"," <tf.Variable 'dense/kernel:0' shape=(8192, 200) dtype=float32_ref>,\n"," <tf.Variable 'dense/bias:0' shape=(200,) dtype=float32_ref>,\n"," <tf.Variable 'batch_normalization_3/gamma:0' shape=(200,) dtype=float32_ref>,\n"," <tf.Variable 'batch_normalization_3/beta:0' shape=(200,) dtype=float32_ref>]\n","INFO:tensorflow:params: 4630218\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Create CheckpointSaverHook.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Saving checkpoints for 0 into model/conve_single_cpu_bs_128_epochs_1_embed_dim_200_dataset_FB15K/model.ckpt.\n","INFO:tensorflow:loss = 1294691.5, step = 0\n","INFO:tensorflow:global_step/sec: 33.8369\n","INFO:tensorflow:loss = 683718.8, step = 100 (2.960 sec)\n","INFO:tensorflow:global_step/sec: 38.878\n","INFO:tensorflow:loss = 216670.5, step = 200 (2.570 sec)\n","INFO:tensorflow:global_step/sec: 38.5212\n","INFO:tensorflow:loss = 117890.17, step = 300 (2.596 sec)\n","INFO:tensorflow:global_step/sec: 38.3141\n","INFO:tensorflow:loss = 72869.22, step = 400 (2.611 sec)\n","INFO:tensorflow:global_step/sec: 38.4615\n","INFO:tensorflow:loss = 53532.688, step = 500 (2.601 sec)\n","INFO:tensorflow:global_step/sec: 38.2746\n","INFO:tensorflow:loss = 39734.434, step = 600 (2.613 sec)\n","INFO:tensorflow:global_step/sec: 38.1756\n","INFO:tensorflow:loss = 36152.867, step = 700 (2.619 sec)\n","INFO:tensorflow:global_step/sec: 38.4053\n","INFO:tensorflow:loss = 23964.793, step = 800 (2.606 sec)\n","INFO:tensorflow:global_step/sec: 37.9884\n","INFO:tensorflow:loss = 26131.293, step = 900 (2.628 sec)\n","INFO:tensorflow:global_step/sec: 38.0956\n","INFO:tensorflow:loss = 20021.734, step = 1000 (2.627 sec)\n","INFO:tensorflow:global_step/sec: 38.0292\n","INFO:tensorflow:loss = 19716.42, step = 1100 (2.631 sec)\n","INFO:tensorflow:global_step/sec: 37.7589\n","INFO:tensorflow:loss = 17645.645, step = 1200 (2.647 sec)\n","INFO:tensorflow:global_step/sec: 38.2231\n","INFO:tensorflow:loss = 15232.516, step = 1300 (2.617 sec)\n","INFO:tensorflow:global_step/sec: 37.9736\n","INFO:tensorflow:loss = 14200.94, step = 1400 (2.634 sec)\n","INFO:tensorflow:global_step/sec: 38.5176\n","INFO:tensorflow:loss = 15055.795, step = 1500 (2.593 sec)\n","INFO:tensorflow:global_step/sec: 37.8887\n","INFO:tensorflow:loss = 14163.942, step = 1600 (2.639 sec)\n","INFO:tensorflow:global_step/sec: 37.8704\n","INFO:tensorflow:loss = 10825.704, step = 1700 (2.643 sec)\n","INFO:tensorflow:global_step/sec: 37.8751\n","INFO:tensorflow:loss = 11276.881, step = 1800 (2.638 sec)\n","INFO:tensorflow:global_step/sec: 37.9277\n","INFO:tensorflow:loss = 13305.038, step = 1900 (2.636 sec)\n","INFO:tensorflow:global_step/sec: 37.8207\n","INFO:tensorflow:loss = 11426.092, step = 2000 (2.644 sec)\n","INFO:tensorflow:global_step/sec: 38.4167\n","INFO:tensorflow:loss = 12778.825, step = 2100 (2.605 sec)\n","INFO:tensorflow:Saving checkpoints for 2126 into model/conve_single_cpu_bs_128_epochs_1_embed_dim_200_dataset_FB15K/model.ckpt.\n","INFO:tensorflow:Loss for final step: 13006.24.\n","INFO:tensorflow:Model Training Ended at Time: 1544179297.285921\n"],"name":"stdout"}]},{"metadata":{"id":"WPmjc79ukmNh","colab_type":"code","outputId":"f7140027-6eb0-492a-fc6b-f7531d401621","executionInfo":{"status":"error","timestamp":1544179092531,"user_tz":-330,"elapsed":742,"user":{"displayName":"Nilesh Agrawal","photoUrl":"","userId":"15528777783423233741"}},"colab":{"base_uri":"https://localhost:8080/","height":273}},"cell_type":"code","source":["tf.logging.info('Evaluation Started at Time: {}'.format(time.time()))\n","metrics = model.evaluate(lambda: eval_input_fn(triples_te, e2idx, p2idx, sp2o_all))\n","tf.logging.info('Evaluation Metrics: {}'.format(metrics))\n","tf.logging.info('Evaluation Loss:{}'.format(metrics['loss']))\n","tf.logging.info('Raw: Mean Rank: {}\\nRaw: Mean Reciprocal Rank: {}\\nRaw: Hits at 1: {}\\nRaw: Hits at 3: {}\\nRaw: Hits at 5: {}\\nRaw: Hits at 10: {}'.format(metrics['mean_rank_raw'],metrics['mean_reciprocal_rank_raw'],metrics['hits_at_1_raw'],metrics['hits_at_3_raw'],metrics['hits_at_5_raw'],metrics['hits_at_10_raw']))\n","tf.logging.info('Filtered: Mean Rank: {}\\nFiltered: Mean Reciprocal Rank: {}\\nFiltered: Hits at 1: {}\\nFiltered: Hits at 3: {}\\nFiltered: Hits at 5: {}\\nFiltered: Hits at 10: {}'.format(metrics['mean_rank_filtered'],metrics['mean_reciprocal_rank_filtered'],metrics['filtered_at_1_raw'],metrics['filtered_at_3_raw'],metrics['filtered_at_5_raw'],metrics['filtered_at_10_raw']))\n","print(metrics)\n","tf.logging.info('Evaluation Ended at Time: {}'.format(time.time()))\n","tf.logging.info('Training Time: {}'.format(training_time))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Evaluation Started at Time: 1544179093.2249198\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-99ccde3d9f7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Evaluation Started at Time: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0meval_input_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtriples_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me2idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp2o_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Evaluation Metrics: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Evaluation Loss:{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Raw: Mean Rank: {}\\nRaw: Mean Reciprocal Rank: {}\\nRaw: Hits at 1: {}\\nRaw: Hits at 3: {}\\nRaw: Hits at 5: {}\\nRaw: Hits at 10: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean_rank_raw'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean_reciprocal_rank_raw'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hits_at_1_raw'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hits_at_3_raw'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hits_at_5_raw'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hits_at_10_raw'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}]},{"metadata":{"id":"i_4RIfJ6knVq","colab_type":"code","colab":{}},"cell_type":"code","source":["'''def evaluate_rank(model, triples_va, triples_te, triples_all, e2idx, p2idx):\n","    for eval_name, eval_triples in [('test', triples_te)]:\n","        _scores_o = list(model.predict(lambda: predict_input_fn(eval_triples, e2idx, p2idx)))\n","        ScoresO = np.reshape(_scores_o, [len(eval_triples), len(e2idx)])\n","        ranks_o, filtered_ranks_o = [], []\n","        for ((s, p, o), scores_o) in tqdm(zip(eval_triples, ScoresO), total=len(eval_triples), ncols=70):\n","            s_idx, p_idx, o_idx = e2idx[s], p2idx[p], e2idx[o]\n","            ranks_o += [1 + np.argsort(np.argsort(- scores_o))[o_idx]]\n","            filtered_scores_o = scores_o.copy()\n","            rm_idx_o = [e2idx[fo] for (fs, fp, fo) in triples_all if fs == s and fp == p and fo != o]\n","            filtered_scores_o[rm_idx_o] = - np.inf\n","            filtered_ranks_o += [1 + np.argsort(np.argsort(- filtered_scores_o))[o_idx]]\n","        for setting_name, setting_ranks in [('Raw', ranks_o), ('Filtered', filtered_ranks_o)]:\n","            mean_rank = np.mean(np.asarray(setting_ranks))\n","            print('[{}] {} MR: {}'.format(eval_name, setting_name, mean_rank))\n","            mean_reciprocal_rank = np.mean(1 / np.asarray(setting_ranks))\n","            print('[{}] {} MRR: {}'.format(eval_name, setting_name, mean_reciprocal_rank))\n","            for k in [1, 3, 5, 10]:\n","                hits_at_k = np.mean(np.asarray(setting_ranks) <= k) * 100\n","                print('[{}] {} Hits@{}: {}'.format(eval_name, setting_name, k, hits_at_k))\n","                \n","evaluate_rank(model,triples_va, triples_te, triples_all, e2idx, p2idx,)'''"],"execution_count":0,"outputs":[]}]}