{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"conve_single_cpu_and_gpu_3layer.ipynb","version":"0.3.2","provenance":[{"file_id":"1FHIhBOd_61BGRcp7Oy33OH5Q4_13XNPj","timestamp":1544179372578},{"file_id":"1Gdny5cRRTAfeLwEDzR48a-DqTviM25fe","timestamp":1544178941888}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"D7PbAsPQJajk","colab_type":"text"},"cell_type":"markdown","source":["1. Run this file: On Single CPU, GPU, and Turing cluster using command \"python filename\".\n","2. Saved the model_dir and output testfor each run after training and testing\n","3. Change parameters model_dir for cpu/gpu and use both DB100K and FB15K for now.\n","4. Output will be three model_dir and output text one for each instances CPU, GPU, turing."]},{"metadata":{"id":"Zh5bYbYMJVj5","colab_type":"code","outputId":"61e74a84-61df-4326-d804-4be73ef7abaa","executionInfo":{"status":"ok","timestamp":1544179391621,"user_tz":-330,"elapsed":5676,"user":{"displayName":"Nilesh Agrawal","photoUrl":"","userId":"15528777783423233741"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["from tqdm import tqdm\n","import tensorflow as tf\n","\n","import numpy as np\n","import sklearn\n","import pprint\n","import itertools\n","import os\n","import time\n","import sys\n","import zipfile\n","# !pip install wget\n","import wget\n","\n","sys.path.append(os.path.dirname(os.getcwd()))\n","tf.logging.set_verbosity(tf.logging.INFO)\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n"],"name":"stdout"}]},{"metadata":{"id":"qsG1vbw2Xwhv","colab_type":"code","colab":{}},"cell_type":"code","source":["class Config:\n","    n_epochs = 100\n","    batch_size = 128\n","    embed_dim = 200\n","    prefetch_buffer_size = None\n","    shuffle_buffer_size = 2\n","    map_threads = 3\n","    data_path = \"data/\"       #constant value\n","    dataset_name=\"FB15K\"      #FB15K/WN18/DB100K     #FB15K 272115, maxlen = 843         # DB100K 597572, maxlen = 85\n","    model_dir = \"model/conve_\"+\"single_gpu\"+\"_bs_\"+str(batch_size)+\"_epochs_\"+str(n_epochs)+\"_embed_dim_\"+str(embed_dim)+\"_dataset_\"+dataset_name # single_cpu / single_gpu"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IDKy5bwxXdy7","colab_type":"code","outputId":"4cf1318d-4e26-4a8b-a08c-e02e5c83fdac","executionInfo":{"status":"ok","timestamp":1544179395474,"user_tz":-330,"elapsed":743,"user":{"displayName":"Nilesh Agrawal","photoUrl":"","userId":"15528777783423233741"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["def download(data_path=Config.data_path, dataset_name=Config.dataset_name):\n","    dataset_path=data_path+dataset_name\n","    if not os.path.exists(dataset_path):\n","      os.makedirs(dataset_path)\n","    dataset_path = dataset_path+'/'\n","    if dataset_name==\"WN18\":\n","      if not os.path.isfile(dataset_path+'train.txt'):\n","          zip_path = dataset_path+\"WN18RR.zip\"\n","          url = \"https://www.dropbox.com/s/sginaquks2xzv6o/WN18RR.zip?dl=1\"\n","          wget.download(url, zip_path)\n","          z = zipfile.ZipFile(zip_path, 'r')\n","          z.extractall(dataset_path)\n","          z = z.close()\n","          os.remove(zip_path)\n","      else:\n","          print('Files Already Downloaded')\n","    elif dataset_name==\"FB15K\":\n","      if not os.path.isfile(dataset_path+'train.txt'):\n","        zip_path = dataset_path+\"FB15K.zip\"\n","        url = \"https://www.dropbox.com/s/kph0mbs79w8itw6/FB15K-237.zip?dl=1\"\n","        wget.download(url, zip_path)\n","        z = zipfile.ZipFile(zip_path, 'r')\n","        z.extractall(dataset_path)\n","        z = z.close()\n","        os.remove(zip_path)\n","      else:\n","          print('Files Already Downloaded')\n","    elif dataset_name==\"DB100K\":\n","      if not os.path.isfile(dataset_path+'train.txt'):\n","        zip_path = dataset_path+\"DB100K.zip\"\n","        url = \"https://www.dropbox.com/s/fmbbh712ilx2zrc/DB100K.zip?dl=1\"\n","        wget.download(url, zip_path)\n","        z = zipfile.ZipFile(zip_path, 'r')\n","        z.extractall(dataset_path)\n","        z = z.close()\n","        os.remove(zip_path)\n","      else:\n","          print('Files Already Downloaded')\n","download()"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Files Already Downloaded\n"],"name":"stdout"}]},{"metadata":{"id":"g7m3a1svXdzB","colab_type":"code","colab":{}},"cell_type":"code","source":["\"\"\"\n","e: entity\n","s: subject\n","p: predicate\n","o: object\n","\"\"\"\n","def glance_dict(d, n=5):\n","    return dict(itertools.islice(d.items(), n))\n","\n","def read_triples(path):\n","    triples = []\n","    with open(path, 'rt') as f:\n","        for line in f.readlines():\n","            s, p, o = line.split()\n","            triples += [(s.strip(), p.strip(), o.strip())]\n","    return triples\n","\n","def load_triple():\n","    download()\n","    dataset_path = Config.data_path+Config.dataset_name+\"/\"\n","    triples_tr = read_triples(dataset_path+'train.txt')\n","    triples_va = read_triples(dataset_path+'valid.txt')\n","    triples_te = read_triples(dataset_path+'test.txt')\n","    triples_all = triples_tr + triples_va + triples_te\n","    return triples_all, triples_tr, triples_va, triples_te\n","\n","\n","def build_vocab(triples):\n","    params = {}\n","    e_set = {s for (s, p, o) in triples} | {o for (s, p, o) in triples}\n","    p_set = {p for (s, p, o) in triples}\n","    params['e_vocab_size'] = len(e_set)\n","    params['p_vocab_size'] = len(p_set)\n","    e2idx = {e: idx for idx, e in enumerate(sorted(e_set))}\n","    p2idx = {p: idx for idx, p in enumerate(sorted(p_set))}\n","    return e2idx, p2idx, params\n","\n","\n","def build_multi_label(triples_tr):\n","    sp2o = {}\n","    for (_s, _p, _o) in triples_tr:\n","        s, p, o = e2idx[_s], p2idx[_p], e2idx[_o] \n","        if (s,p) not in sp2o:\n","            sp2o[(s,p)] = [o]\n","        else:\n","            if o not in sp2o[(s,p)]:\n","                sp2o[(s,p)].append(o)\n","    return sp2o\n","  \n","def build_multi_label_all(triples_all):\n","    sp2o_all = {}\n","    for (_s, _p, _o) in triples_all:\n","        s, p, o = e2idx[_s], p2idx[_p], e2idx[_o] \n","        if (s,p) not in sp2o_all:\n","            sp2o_all[(s,p)] = [o]\n","        else:\n","            if o not in sp2o_all[(s,p)]:\n","                sp2o_all[(s,p)].append(o)\n","    return sp2o_all  \n","\n","\n","def get_train_y(triples_tr, e2idx, p2idx, sp2o):\n","    y = []\n","    for (_s, _p, _o) in triples_tr:\n","        s, p, o = e2idx[_s], p2idx[_p], e2idx[_o] \n","        temp = np.zeros([len(e2idx)])\n","        temp[sp2o[(s,p)]] = 1.\n","        y.append(temp)\n","    y = np.asarray(y)\n","    return y\n","\n","def get_eval_y(triples_te, e2idx, p2idx, sp2o_all):\n","    y = []\n","    for (_s, _p, _o) in triples_te:\n","        s, p, o = e2idx[_s], p2idx[_p], e2idx[_o] \n","        temp1 = np.zeros([len(e2idx)])\n","        temp1[o] = 1.\n","        temp2 = np.ones([len(e2idx)])\n","        temp2[sp2o_all[(s,p)]] = -1.\n","        temp2[o] = 1.\n","        y.append((temp1,temp2))\n","    y = np.asarray(y)\n","    return y \n","  \n","def get_features_labels(triples_tr, e2idx, p2idx, sp2o,entity_vocab_size):\n","  features = [[e2idx[s],p2idx[p]] for (s, p, o) in triples_tr]\n","  labels = [sp2o[(e2idx[s],p2idx[p])] for (s, p, o) in triples_tr]\n","  new_labels = np.full([len(labels),len(max(labels,key = lambda x: len(x)))],params['e_vocab_size']+1)\n","  for i,j in enumerate(labels):\n","    new_labels[i][0:len(j)] = j\n","  return features, new_labels"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GejH61frLgnT","colab_type":"code","colab":{}},"cell_type":"code","source":["def next_train_batch(triples_tr, e2idx, p2idx, sp2o):\n","    for i in range(0, len(triples_tr), Config.batch_size):\n","        _triples_tr = triples_tr[i: i+Config.batch_size]\n","        x_s = np.asarray([e2idx[s] for (s, p, o) in _triples_tr], dtype=np.int32)\n","        x_p = np.asarray([p2idx[p] for (s, p, o) in _triples_tr], dtype=np.int32)\n","        y = get_train_y(_triples_tr, e2idx, p2idx, sp2o)\n","        yield ({'s': x_s, 'p': x_p}, y)\n","\n","def train_input_fn(triples_tr, e2idx, p2idx, sp2o):\n","    dataset = tf.data.Dataset.from_generator(\n","        lambda: next_train_batch(triples_tr,\n","                                 e2idx,\n","                                 p2idx,\n","                                 sp2o),\n","        ({'s': tf.int32, 'p': tf.int32}, tf.float32),\n","        ({'s': tf.TensorShape([None]), 'p': tf.TensorShape([None])},\n","         tf.TensorShape([None, len(e2idx)])))\n","    #dataset = dataset.shard(num_shards= ,  index = )\n","    dataset = dataset.shuffle(buffer_size=Config.shuffle_buffer_size)\n","    dataset = dataset.repeat(Config.n_epochs)\n","    #dataset = dataset.batch(Config.batch_size)\n","    dataset = dataset.prefetch(Config.prefetch_buffer_size)\n","    return dataset\n","  \n","def next_train_single(triples_tr, e2idx, p2idx, sp2o):\n","    for i in range(0, len(triples_tr)):\n","        s,p,o = triples_tr[i]\n","        x_s = e2idx[s]\n","        x_p = p2idx[p] \n","        y = np.zeros([len(e2idx)])\n","        y[sp2o[(x_s,x_p)]] = 1.\n","        yield ({'s': x_s, 'p': x_p}, y)\n","        \n","def train_input_fn_single(triples_tr, e2idx, p2idx, sp2o):\n","    dataset = tf.data.Dataset.from_generator(\n","        lambda: next_train_single(triples_tr,\n","                                 e2idx,\n","                                 p2idx,\n","                                 sp2o),\n","        ({'s': tf.int32, 'p': tf.int32}, tf.float32),\n","        ({'s': tf.TensorShape([]), 'p': tf.TensorShape([])},\n","         tf.TensorShape([len(e2idx)])))\n","    #dataset = dataset.shard(num_shards= ,  index = )\n","    dataset = dataset.shuffle(buffer_size=Config.shuffle_buffer_size)\n","    dataset = dataset.repeat(Config.n_epochs)\n","    dataset = dataset.batch(Config.batch_size)\n","    dataset = dataset.prefetch(Config.prefetch_buffer_size)\n","    return dataset\n","\n","def next_eval_batch(triples_te, e2idx, p2idx, sp2o_all):\n","    for i in range(0, len(triples_te), Config.batch_size):\n","        _triples_te = triples_te[i: i+Config.batch_size]\n","        x_s = np.asarray([e2idx[s] for (s, p, o) in _triples_te], dtype=np.int32)\n","        x_p = np.asarray([p2idx[p] for (s, p, o) in _triples_te], dtype=np.int32)\n","        y = get_eval_y(_triples_te, e2idx, p2idx, sp2o_all)\n","        yield ({'s': x_s, 'p': x_p}, y)\n","  \n","def eval_input_fn(triples_te, e2idx, p2idx, sp2o_all):\n","    dataset = tf.data.Dataset.from_generator(\n","        lambda: next_eval_batch(triples_te,\n","                             e2idx, \n","                             p2idx,\n","                             sp2o_all),\n","        ({'s': tf.int32, 'p': tf.int32}, tf.float32),\n","        ({'s': tf.TensorShape([None]), 'p': tf.TensorShape([None])},\n","         tf.TensorShape([None, 2, len(e2idx)])))\n","    return dataset.prefetch(1)\n","  \n","def next_predict_batch(triples, e2idx, p2idx):\n","    for i in range(0, len(triples), Config.batch_size):\n","        _triples = triples[i: i+Config.batch_size]\n","        x_s = np.asarray([e2idx[s] for (s, p, o) in _triples], dtype=np.int32)\n","        x_p = np.asarray([p2idx[p] for (s, p, o) in _triples], dtype=np.int32)\n","        yield {'s': x_s, 'p': x_p}\n","  \n","def predict_input_fn(triples,\n","               e2idx, \n","               p2idx):\n","    dataset = tf.data.Dataset.from_generator(\n","        lambda: next_predict_batch(triples,\n","                             e2idx, \n","                             p2idx),\n","        ({'s':tf.int32, 'p':tf.int32}),\n","        ({'s':tf.TensorShape([None]),\n","         'p':tf.TensorShape([None])}))\n","    return dataset.prefetch(1)  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"0JmmEkJfXdzP","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","def tf_get_rank_order(input, targets, filtered):\n","      target1 = targets[:,0]\n","      target2 = targets[:,1]\n","      tf.logging.info\n","      size = tf.shape(input)[-1]\n","      if filtered:\n","        filtered_input = tf.multiply(input,target2)\n","        indices_of_ranks = tf.nn.top_k(-filtered_input, k=size)[1]\n","      else:\n","        indices_of_ranks = tf.nn.top_k(-input, k=size)[1]\n","      indices_of_ranks = size - tf.nn.top_k(-indices_of_ranks, k=size)[1]\n","      indices_of_o = tf.cast(tf.argmax(target1, axis=1),tf.int32)\n","      row_indices = tf.range(tf.shape(indices_of_o)[0])\n","      full_indices = tf.stack([row_indices, indices_of_o], axis=1)\n","      return tf.gather_nd(indices_of_ranks, full_indices)\n","\n","def get_rank(logits, targets, filtered = False):\n","    ordered_array = tf_get_rank_order(logits, targets, filtered)\n","    return ordered_array"],"execution_count":0,"outputs":[]},{"metadata":{"id":"960UHskCXdzU","colab_type":"code","colab":{}},"cell_type":"code","source":["def forward(features, mode, params):\n","    batch_sz = tf.shape(features['s'])[0]\n","    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n","    \n","    e_embed = tf.get_variable('e_embed',\n","                              [params['e_vocab_size'], Config.embed_dim],\n","                              initializer=tf.variance_scaling_initializer())\n","    p_embed = tf.get_variable('p_embed',\n","                              [params['p_vocab_size'], Config.embed_dim],\n","                              initializer=tf.variance_scaling_initializer())\n","    \n","    s = tf.nn.embedding_lookup(e_embed, features['s'])\n","    p = tf.nn.embedding_lookup(p_embed, features['p'])\n","    \n","    x = tf.concat([tf.reshape(s, [batch_sz, 10, 20, 1]),\n","                   tf.reshape(p, [batch_sz, 10, 20, 1]),], axis=1)\n","    x = tf.layers.batch_normalization(x, training=is_training)\n","    x = tf.layers.dropout(x, 0.2, training=is_training)\n","    \n","    x = tf.layers.conv2d(x, 64, (5,5), activation=tf.nn.relu)\n","    x = tf.layers.batch_normalization(x, training=is_training)\n","    x = tf.layers.dropout(x, 0.2, training=is_training)\n","\n","    x = tf.layers.conv2d(x, 32, (3,3), activation=tf.nn.relu)\n","    x = tf.layers.batch_normalization(x, training=is_training)\n","    x = tf.layers.dropout(x, 0.2, training=is_training)\n","    \n","    x = tf.layers.conv2d(x, 16, (3,3), activation=tf.nn.relu)\n","    x = tf.layers.flatten(x)\n","    x = tf.layers.batch_normalization(x, training=is_training)\n","    x = tf.layers.dropout(x, 0.2, training=is_training)\n","    \n","    x = tf.layers.dense(x, Config.embed_dim, tf.nn.relu)\n","    x = tf.layers.batch_normalization(x, training=is_training)\n","    x = tf.layers.dropout(x, 0.3, training=is_training)\n","    \n","    logits = tf.matmul(x, e_embed, transpose_b=True)\n","    return logits\n","    \n","    \n","def model_fn(features, labels, mode, params):\n","    print(\"Shape of features: {} and labels: {}\".format(features['s'].shape,labels.shape))\n","    logits = forward(features, mode, params)\n","    \n","    if mode == tf.estimator.ModeKeys.TRAIN:\n","        tf.logging.info('\\n'+pprint.pformat(tf.trainable_variables()))\n","        tf.logging.info('params: %d'%count_train_params())\n","        \n","        global_step = tf.train.get_global_step()\n","        \n","        loss_op = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,\n","                                                                        labels=labels))\n","        \n","        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n","            train_op = tf.train.AdamOptimizer().minimize(loss_op,\n","                                                         global_step = global_step)\n","        \n","        return tf.estimator.EstimatorSpec(mode = mode,\n","                                          loss = loss_op,\n","                                          train_op = train_op)\n","    \n","    if mode == tf.estimator.ModeKeys.PREDICT:\n","        return tf.estimator.EstimatorSpec(mode = mode, predictions = tf.sigmoid(logits))\n","      \n","    if mode == tf.estimator.ModeKeys.EVAL:\n","        filtered_rank = tf.cast(get_rank(tf.sigmoid(logits), labels, filtered = True), tf.float32)\n","        raw_rank = tf.cast(get_rank(tf.sigmoid(logits), labels, filtered = False), tf.float32)\n","        metrics = {\n","          'mean_rank_raw': tf.metrics.mean(raw_rank),\n","          'mean_rank_filtered': tf.metrics.mean(filtered_rank),\n","          'mean_reciprocal_rank_raw': tf.metrics.mean(1./raw_rank),\n","          'mean_reciprocal_rank_filtered': tf.metrics.mean(1./filtered_rank),\n","          'hits_at_1_raw' : tf.metrics.mean(raw_rank <= 1.),\n","          'hits_at_3_raw' : tf.metrics.mean(raw_rank <= 3.),\n","          'hits_at_5_raw' : tf.metrics.mean(raw_rank <= 5.),\n","          'hits_at_10_raw' : tf.metrics.mean(raw_rank <= 10.),\n","          'filtered_at_1_raw' : tf.metrics.mean(filtered_rank <= 1.),\n","          'filtered_at_3_raw' : tf.metrics.mean(filtered_rank <= 3.),\n","          'filtered_at_5_raw' : tf.metrics.mean(filtered_rank <= 5.),\n","          'filtered_at_10_raw' : tf.metrics.mean(filtered_rank <= 10.)\n","        }\n","        loss_op = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,\n","                                                                        labels=labels[:,0]))\n","        return tf.estimator.EstimatorSpec(mode = mode,loss=loss_op, eval_metric_ops=metrics)\n","\n","def count_train_params():\n","    return np.sum([np.prod([d.value for d in v.get_shape()]) for v in tf.trainable_variables()])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OqSsh25dMsY5","colab_type":"code","outputId":"b931a553-b5c1-4255-ca5c-d8ebee234c0f","executionInfo":{"status":"ok","timestamp":1544179443248,"user_tz":-330,"elapsed":2984,"user":{"displayName":"Nilesh Agrawal","photoUrl":"","userId":"15528777783423233741"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["triples_all, triples_tr, triples_va, triples_te = load_triple()\n","e2idx, p2idx, params = build_vocab(triples_all)\n","sp2o = build_multi_label(triples_tr)\n","sp2o_all = build_multi_label_all(triples_all)\n","# features, labels = get_features_labels(triples_tr, e2idx, p2idx, sp2o,params['e_vocab_size'] )"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Files Already Downloaded\n"],"name":"stdout"}]},{"metadata":{"id":"vV8eRVy-M07E","colab_type":"code","outputId":"d609b00b-3dcc-44c5-d922-65a533280e4d","executionInfo":{"status":"ok","timestamp":1544179601011,"user_tz":-330,"elapsed":61773,"user":{"displayName":"Nilesh Agrawal","photoUrl":"","userId":"15528777783423233741"}},"colab":{"base_uri":"https://localhost:8080/","height":1496}},"cell_type":"code","source":["config = tf.estimator.RunConfig(model_dir=Config.model_dir)\n","model = tf.estimator.Estimator(model_fn,\n","                               params = params,\n","                              config = config)\n","training_time = time.time()\n","tf.logging.info('Model Training Started at Time: {}'.format(time.time()))\n","model.train(lambda: train_input_fn(triples_tr, e2idx, p2idx, sp2o))\n","tf.logging.info('Model Training Ended at Time: {}'.format(time.time()))\n","training_time = time.time() - training_time "],"execution_count":13,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Using config: {'_model_dir': 'model/conve_single_cpu_bs_128_epochs_1_embed_dim_200_dataset_FB15K', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ffa5a78ce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n","INFO:tensorflow:Model Training Started at Time: 1544179540.6903284\n","INFO:tensorflow:Calling model_fn.\n","Shape of features: (?,) and labels: (?, 14541)\n","INFO:tensorflow:\n","[<tf.Variable 'e_embed:0' shape=(14541, 200) dtype=float32_ref>,\n"," <tf.Variable 'p_embed:0' shape=(237, 200) dtype=float32_ref>,\n"," <tf.Variable 'batch_normalization/gamma:0' shape=(1,) dtype=float32_ref>,\n"," <tf.Variable 'batch_normalization/beta:0' shape=(1,) dtype=float32_ref>,\n"," <tf.Variable 'conv2d/kernel:0' shape=(5, 5, 1, 64) dtype=float32_ref>,\n"," <tf.Variable 'conv2d/bias:0' shape=(64,) dtype=float32_ref>,\n"," <tf.Variable 'batch_normalization_1/gamma:0' shape=(64,) dtype=float32_ref>,\n"," <tf.Variable 'batch_normalization_1/beta:0' shape=(64,) dtype=float32_ref>,\n"," <tf.Variable 'conv2d_1/kernel:0' shape=(3, 3, 64, 32) dtype=float32_ref>,\n"," <tf.Variable 'conv2d_1/bias:0' shape=(32,) dtype=float32_ref>,\n"," <tf.Variable 'batch_normalization_2/gamma:0' shape=(32,) dtype=float32_ref>,\n"," <tf.Variable 'batch_normalization_2/beta:0' shape=(32,) dtype=float32_ref>,\n"," <tf.Variable 'conv2d_2/kernel:0' shape=(3, 3, 32, 16) dtype=float32_ref>,\n"," <tf.Variable 'conv2d_2/bias:0' shape=(16,) dtype=float32_ref>,\n"," <tf.Variable 'batch_normalization_3/gamma:0' shape=(2304,) dtype=float32_ref>,\n"," <tf.Variable 'batch_normalization_3/beta:0' shape=(2304,) dtype=float32_ref>,\n"," <tf.Variable 'dense/kernel:0' shape=(2304, 200) dtype=float32_ref>,\n"," <tf.Variable 'dense/bias:0' shape=(200,) dtype=float32_ref>,\n"," <tf.Variable 'batch_normalization_4/gamma:0' shape=(200,) dtype=float32_ref>,\n"," <tf.Variable 'batch_normalization_4/beta:0' shape=(200,) dtype=float32_ref>]\n","INFO:tensorflow:params: 3446554\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Create CheckpointSaverHook.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Saving checkpoints for 0 into model/conve_single_cpu_bs_128_epochs_1_embed_dim_200_dataset_FB15K/model.ckpt.\n","INFO:tensorflow:loss = 1294585.8, step = 0\n","INFO:tensorflow:global_step/sec: 33.897\n","INFO:tensorflow:loss = 717659.6, step = 100 (2.954 sec)\n","INFO:tensorflow:global_step/sec: 38.1383\n","INFO:tensorflow:loss = 217335.38, step = 200 (2.623 sec)\n","INFO:tensorflow:global_step/sec: 38.4221\n","INFO:tensorflow:loss = 116145.35, step = 300 (2.603 sec)\n","INFO:tensorflow:global_step/sec: 38.2022\n","INFO:tensorflow:loss = 66290.19, step = 400 (2.617 sec)\n","INFO:tensorflow:global_step/sec: 38.2495\n","INFO:tensorflow:loss = 51978.773, step = 500 (2.613 sec)\n","INFO:tensorflow:global_step/sec: 38.1126\n","INFO:tensorflow:loss = 35234.63, step = 600 (2.627 sec)\n","INFO:tensorflow:global_step/sec: 37.9721\n","INFO:tensorflow:loss = 34529.21, step = 700 (2.633 sec)\n","INFO:tensorflow:global_step/sec: 38.0705\n","INFO:tensorflow:loss = 25702.627, step = 800 (2.625 sec)\n","INFO:tensorflow:global_step/sec: 37.7658\n","INFO:tensorflow:loss = 26589.578, step = 900 (2.646 sec)\n","INFO:tensorflow:global_step/sec: 37.8804\n","INFO:tensorflow:loss = 19795.88, step = 1000 (2.644 sec)\n","INFO:tensorflow:global_step/sec: 38.0009\n","INFO:tensorflow:loss = 24140.117, step = 1100 (2.631 sec)\n","INFO:tensorflow:global_step/sec: 38.103\n","INFO:tensorflow:loss = 17200.342, step = 1200 (2.625 sec)\n","INFO:tensorflow:global_step/sec: 37.9297\n","INFO:tensorflow:loss = 15317.842, step = 1300 (2.632 sec)\n","INFO:tensorflow:global_step/sec: 37.7704\n","INFO:tensorflow:loss = 15416.863, step = 1400 (2.653 sec)\n","INFO:tensorflow:global_step/sec: 37.1778\n","INFO:tensorflow:loss = 16046.055, step = 1500 (2.688 sec)\n","INFO:tensorflow:global_step/sec: 37.4163\n","INFO:tensorflow:loss = 12932.81, step = 1600 (2.670 sec)\n","INFO:tensorflow:global_step/sec: 37.2672\n","INFO:tensorflow:loss = 16173.287, step = 1700 (2.682 sec)\n","INFO:tensorflow:global_step/sec: 37.3804\n","INFO:tensorflow:loss = 15944.742, step = 1800 (2.678 sec)\n","INFO:tensorflow:global_step/sec: 36.6715\n","INFO:tensorflow:loss = 11910.16, step = 1900 (2.727 sec)\n","INFO:tensorflow:global_step/sec: 37.918\n","INFO:tensorflow:loss = 12383.643, step = 2000 (2.639 sec)\n","INFO:tensorflow:global_step/sec: 37.1848\n","INFO:tensorflow:loss = 10889.995, step = 2100 (2.689 sec)\n","INFO:tensorflow:Saving checkpoints for 2126 into model/conve_single_cpu_bs_128_epochs_1_embed_dim_200_dataset_FB15K/model.ckpt.\n","INFO:tensorflow:Loss for final step: 12016.292.\n","INFO:tensorflow:Model Training Ended at Time: 1544179601.9452856\n"],"name":"stdout"}]},{"metadata":{"id":"WPmjc79ukmNh","colab_type":"code","outputId":"820d2057-3544-41b5-b657-589203b957ff","executionInfo":{"status":"ok","timestamp":1544179641886,"user_tz":-330,"elapsed":8341,"user":{"displayName":"Nilesh Agrawal","photoUrl":"","userId":"15528777783423233741"}},"colab":{"base_uri":"https://localhost:8080/","height":541}},"cell_type":"code","source":["tf.logging.info('Evaluation Started at Time: {}'.format(time.time()))\n","metrics = model.evaluate(lambda: eval_input_fn(triples_te, e2idx, p2idx, sp2o_all))\n","tf.logging.info('Evaluation Metrics: {}'.format(metrics))\n","tf.logging.info('Evaluation Loss:{}'.format(metrics['loss']))\n","tf.logging.info('Raw: Mean Rank: {}\\nRaw: Mean Reciprocal Rank: {}\\nRaw: Hits at 1: {}\\nRaw: Hits at 3: {}\\nRaw: Hits at 5: {}\\nRaw: Hits at 10: {}'.format(metrics['mean_rank_raw'],metrics['mean_reciprocal_rank_raw'],metrics['hits_at_1_raw'],metrics['hits_at_3_raw'],metrics['hits_at_5_raw'],metrics['hits_at_10_raw']))\n","tf.logging.info('Filtered: Mean Rank: {}\\nFiltered: Mean Reciprocal Rank: {}\\nFiltered: Hits at 1: {}\\nFiltered: Hits at 3: {}\\nFiltered: Hits at 5: {}\\nFiltered: Hits at 10: {}'.format(metrics['mean_rank_filtered'],metrics['mean_reciprocal_rank_filtered'],metrics['filtered_at_1_raw'],metrics['filtered_at_3_raw'],metrics['filtered_at_5_raw'],metrics['filtered_at_10_raw']))\n","print(metrics)\n","tf.logging.info('Evaluation Ended at Time: {}'.format(time.time()))\n","tf.logging.info('Training Time: {}'.format(training_time))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Evaluation Started at Time: 1544179635.0068936\n","INFO:tensorflow:Calling model_fn.\n","Shape of features: (?,) and labels: (?, 2, 14541)\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Starting evaluation at 2018-12-07-10:47:15\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from model/conve_single_cpu_bs_128_epochs_1_embed_dim_200_dataset_FB15K/model.ckpt-2126\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Finished evaluation at 2018-12-07-10:47:22\n","INFO:tensorflow:Saving dict for global step 2126: filtered_at_10_raw = 0.31031954, filtered_at_1_raw = 0.15513535, filtered_at_3_raw = 0.22349262, filtered_at_5_raw = 0.25505716, global_step = 2126, hits_at_10_raw = 0.23580572, hits_at_1_raw = 0.10451481, hits_at_3_raw = 0.15313202, hits_at_5_raw = 0.18127626, loss = 9584.237, mean_rank_filtered = 1949.2825, mean_rank_raw = 1973.6218, mean_reciprocal_rank_filtered = 0.20679656, mean_reciprocal_rank_raw = 0.14819984\n","INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2126: model/conve_single_cpu_bs_128_epochs_1_embed_dim_200_dataset_FB15K/model.ckpt-2126\n","INFO:tensorflow:Evaluation Metrics: {'filtered_at_10_raw': 0.31031954, 'filtered_at_1_raw': 0.15513535, 'filtered_at_3_raw': 0.22349262, 'filtered_at_5_raw': 0.25505716, 'hits_at_10_raw': 0.23580572, 'hits_at_1_raw': 0.10451481, 'hits_at_3_raw': 0.15313202, 'hits_at_5_raw': 0.18127626, 'loss': 9584.237, 'mean_rank_filtered': 1949.2825, 'mean_rank_raw': 1973.6218, 'mean_reciprocal_rank_filtered': 0.20679656, 'mean_reciprocal_rank_raw': 0.14819984, 'global_step': 2126}\n","INFO:tensorflow:Evaluation Loss:9584.2373046875\n","INFO:tensorflow:Raw: Mean Rank: 1973.621826171875\n","Raw: Mean Reciprocal Rank: 0.14819984138011932\n","Raw: Hits at 1: 0.10451480746269226\n","Raw: Hits at 3: 0.15313202142715454\n","Raw: Hits at 5: 0.18127626180648804\n","Raw: Hits at 10: 0.2358057200908661\n","INFO:tensorflow:Filtered: Mean Rank: 1949.282470703125\n","Filtered: Mean Reciprocal Rank: 0.2067965567111969\n","Filtered: Hits at 1: 0.1551353484392166\n","Filtered: Hits at 3: 0.22349262237548828\n","Filtered: Hits at 5: 0.255057156085968\n","Filtered: Hits at 10: 0.31031954288482666\n","{'filtered_at_10_raw': 0.31031954, 'filtered_at_1_raw': 0.15513535, 'filtered_at_3_raw': 0.22349262, 'filtered_at_5_raw': 0.25505716, 'hits_at_10_raw': 0.23580572, 'hits_at_1_raw': 0.10451481, 'hits_at_3_raw': 0.15313202, 'hits_at_5_raw': 0.18127626, 'loss': 9584.237, 'mean_rank_filtered': 1949.2825, 'mean_rank_raw': 1973.6218, 'mean_reciprocal_rank_filtered': 0.20679656, 'mean_reciprocal_rank_raw': 0.14819984, 'global_step': 2126}\n","INFO:tensorflow:Evaluation Ended at Time: 1544179642.6444082\n","INFO:tensorflow:Training Time: 61.25735116004944\n"],"name":"stdout"}]},{"metadata":{"id":"i_4RIfJ6knVq","colab_type":"code","colab":{}},"cell_type":"code","source":["'''def evaluate_rank(model, triples_va, triples_te, triples_all, e2idx, p2idx):\n","    for eval_name, eval_triples in [('test', triples_te)]:\n","        _scores_o = list(model.predict(lambda: predict_input_fn(eval_triples, e2idx, p2idx)))\n","        ScoresO = np.reshape(_scores_o, [len(eval_triples), len(e2idx)])\n","        ranks_o, filtered_ranks_o = [], []\n","        for ((s, p, o), scores_o) in tqdm(zip(eval_triples, ScoresO), total=len(eval_triples), ncols=70):\n","            s_idx, p_idx, o_idx = e2idx[s], p2idx[p], e2idx[o]\n","            ranks_o += [1 + np.argsort(np.argsort(- scores_o))[o_idx]]\n","            filtered_scores_o = scores_o.copy()\n","            rm_idx_o = [e2idx[fo] for (fs, fp, fo) in triples_all if fs == s and fp == p and fo != o]\n","            filtered_scores_o[rm_idx_o] = - np.inf\n","            filtered_ranks_o += [1 + np.argsort(np.argsort(- filtered_scores_o))[o_idx]]\n","        for setting_name, setting_ranks in [('Raw', ranks_o), ('Filtered', filtered_ranks_o)]:\n","            mean_rank = np.mean(np.asarray(setting_ranks))\n","            print('[{}] {} MR: {}'.format(eval_name, setting_name, mean_rank))\n","            mean_reciprocal_rank = np.mean(1 / np.asarray(setting_ranks))\n","            print('[{}] {} MRR: {}'.format(eval_name, setting_name, mean_reciprocal_rank))\n","            for k in [1, 3, 5, 10]:\n","                hits_at_k = np.mean(np.asarray(setting_ranks) <= k) * 100\n","                print('[{}] {} Hits@{}: {}'.format(eval_name, setting_name, k, hits_at_k))\n","                \n","evaluate_rank(model,triples_va, triples_te, triples_all, e2idx, p2idx,)'''"],"execution_count":0,"outputs":[]}]}