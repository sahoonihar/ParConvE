{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"distmult_distributed_parameter_server.ipynb","version":"0.3.2","provenance":[{"file_id":"16fLW1Ck8uR22lx8un0ryT1OtW4EaP8S1","timestamp":1544202879295},{"file_id":"1Jc8rFThAR0qNAD9LBu1bsOj7HNt9E37r","timestamp":1543522742222}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"D7PbAsPQJajk","colab_type":"text"},"cell_type":"markdown","source":["1. Run this file: On GPU, using command \"python filename\".\n","2. Saved the model_dir and output testfor each run after training and testing\n","3. Change parameters num_gpu and use only FB15K for now.\n","4. Output will be four model_dir and output text one for each gpu spec 1/2/4/8."]},{"metadata":{"id":"Zh5bYbYMJVj5","colab_type":"code","outputId":"89007335-3741-4acf-b6d4-ad6a09737666","executionInfo":{"status":"ok","timestamp":1543927601611,"user_tz":-330,"elapsed":3168,"user":{"displayName":"Nilesh Agrawal","photoUrl":"","userId":"15528777783423233741"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["from tqdm import tqdm\n","import tensorflow as tf\n","import numpy as np\n","import sklearn\n","import pprint\n","import itertools\n","import os\n","import time\n","import sys\n","import zipfile\n","#!pip install wget\n","import wget\n","import argparse\n","import json\n","\n","sys.path.append(os.path.dirname(os.getcwd()))\n","tf.logging.set_verbosity(tf.logging.INFO)\n","# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n"],"name":"stdout"}]},{"metadata":{"id":"VXqyae0HLyNd","colab_type":"code","outputId":"be689768-7043-4c28-c009-74f95d51be54","executionInfo":{"status":"error","timestamp":1543927601614,"user_tz":-330,"elapsed":2630,"user":{"displayName":"Nilesh Agrawal","photoUrl":"","userId":"15528777783423233741"}},"colab":{"base_uri":"https://localhost:8080/","height":201}},"cell_type":"code","source":["ps = ['node15:3333']\n","chief = ['node16:3333']\n","worker = ['node17:3333','node18:3333','node19:3333' ]\n","cluster = {'chief' : chief,\n","            'ps' : ps,\n","             'worker' : worker}\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument(\"--type\", \"-t\", help=\"set node type (worker/ps/chief)\")\n","parser.add_argument(\"--index\", \"-i\", help=\"set node index\")\n","parser.add_argument(\"--strategy\", \"-s\", help=\"set strategy (sync/async/ssp)\")\n","args = parser.parse_args()\n","\n","os.environ['TF_CONFIG'] = json.dumps(\n","    {'cluster': cluster,\n","     'task': {'type': args.type, 'index': int(args.index)}})    \n","\n","NUM_SHARDS = len(chief)+len(worker)\n","SHARD_INDEX = int(args.index) if args.type == 'worker' else NUM_SHARDS -1\n","PS_STRATEGY = args.strategy"],"execution_count":0,"outputs":[{"output_type":"stream","text":["usage: ipykernel_launcher.py [-h] [--type TYPE] [--index INDEX]\n","                             [--strategy STRATEGY]\n","ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-4d34bc8e-11eb-4a8c-beec-bdf9976d7030.json\n"],"name":"stderr"},{"output_type":"error","ename":"SystemExit","evalue":"ignored","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"]},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"],"name":"stderr"}]},{"metadata":{"id":"b_GYJiFa1C0I","colab_type":"code","outputId":"fb161815-a642-450f-ca93-8a130a08659c","executionInfo":{"status":"error","timestamp":1543927603780,"user_tz":-330,"elapsed":819,"user":{"displayName":"Nilesh Agrawal","photoUrl":"","userId":"15528777783423233741"}},"colab":{"base_uri":"https://localhost:8080/","height":360}},"cell_type":"code","source":["class Config:\n","  n_epochs = 10\n","  batch_size = 128\n","  embed_dim = 200\n","  prefetch_buffer_size = None\n","  shuffle_buffer_size = 2\n","  map_threads = 3\n","  staleness = 30\n","  data_path = \"data/\"       #constant value\n","  dataset_name=\"FB15K\"      #FB15K/WN18/DB100K     #FB15K 272115, maxlen = 843         # DB100K 597572, maxlen = 85\n","  num_steps = 2120 if dataset_name == 'FB15K' else 4660\n","  num_workers = len(worker) + len(chief)\n","  num_ps = len(ps)\n","  is_chief = True if args.type == 'chief' else False\n","  model_dir = \"model/distmult_\"+\"distributed_parameter_server\"+\"_strategy_\"+PS_STRATEGY+\"_ps_\"+str(num_ps)+\"_workers_\"+str(num_workers)+\"_bs_\"+str(batch_size)+\"_epochs_\"+str(n_epochs)+\"_embed_dim_\"+str(embed_dim)+\"_dataset_\"+dataset_name\n","  #num_gpus_per_worker = 0\n","  #model_dir = \"model/distmult_\"+\"distributed_parameter_server_gpu_\"+\"_strategy_\"+PS_STRATEGY+\"_num_gpu_per_worker_\"+str(num_gpus_per_worker)+\"_ps_\"+str(num_ps)+\"_workers_\"+str(num_workers)+\"_bs_\"+str(batch_size)+\"_epochs_\"+str(n_epochs)+\"_embed_dim_\"+str(embed_dim)+\"_dataset_\"+dataset_name"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-85712d10ac65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0membed_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprefetch_buffer_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-85712d10ac65>\u001b[0m in \u001b[0;36mConfig\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchief\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mnum_ps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mis_chief\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'chief'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0mmodel_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model/conve_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"distributed_parameter_server\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_strategy_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mPS_STRATEGY\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_ps_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_ps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_workers_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_bs_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_epochs_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_embed_dim_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_dataset_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"]}]},{"metadata":{"id":"qsG1vbw2Xwhv","colab_type":"code","outputId":"05eb83fe-00f6-436b-b581-6cb3fc088f2a","executionInfo":{"status":"error","timestamp":1543927637532,"user_tz":-330,"elapsed":1055,"user":{"displayName":"Nilesh Agrawal","photoUrl":"","userId":"15528777783423233741"}},"colab":{"base_uri":"https://localhost:8080/","height":236}},"cell_type":"code","source":["def download(data_path=Config.data_path, dataset_name=Config.dataset_name):\n","    dataset_path=data_path+dataset_name\n","    if not os.path.exists(dataset_path):\n","      os.makedirs(dataset_path)\n","    dataset_path = dataset_path+'/'\n","    if dataset_name==\"WN18\":\n","      if not os.path.isfile(dataset_path+'train.txt'):\n","          zip_path = dataset_path+\"WN18RR.zip\"\n","          url = \"https://www.dropbox.com/s/sginaquks2xzv6o/WN18RR.zip?dl=1\"\n","          wget.download(url, zip_path)\n","          z = zipfile.ZipFile(zip_path, 'r')\n","          z.extractall(dataset_path)\n","          z = z.close()\n","          os.remove(zip_path)\n","      else:\n","          print('Files Already Downloaded')\n","    elif dataset_name==\"FB15K\":\n","      if not os.path.isfile(dataset_path+'train.txt'):\n","        zip_path = dataset_path+\"FB15K.zip\"\n","        url = \"https://www.dropbox.com/s/kph0mbs79w8itw6/FB15K-237.zip?dl=1\"\n","        wget.download(url, zip_path)\n","        z = zipfile.ZipFile(zip_path, 'r')\n","        z.extractall(dataset_path)\n","        z = z.close()\n","        os.remove(zip_path)\n","      else:\n","          print('Files Already Downloaded')\n","    elif dataset_name==\"DB100K\":\n","      if not os.path.isfile(dataset_path+'train.txt'):\n","        zip_path = dataset_path+\"DB100K.zip\"\n","        url = \"https://www.dropbox.com/s/fmbbh712ilx2zrc/DB100K.zip?dl=1\"\n","        wget.download(url, zip_path)\n","        z = zipfile.ZipFile(zip_path, 'r')\n","        z.extractall(dataset_path)\n","        z = z.close()\n","        os.remove(zip_path)\n","      else:\n","          print('Files Already Downloaded')\n","download()"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-371a5954269b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdataset_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdataset_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Config' is not defined"]}]},{"metadata":{"id":"g7m3a1svXdzB","colab_type":"code","colab":{}},"cell_type":"code","source":["\"\"\"\n","e: entity\n","s: subject\n","p: predicate\n","o: object\n","\"\"\"\n","def glance_dict(d, n=5):\n","    return dict(itertools.islice(d.items(), n))\n","\n","def read_triples(path):\n","    triples = []\n","    with open(path, 'rt') as f:\n","        for line in f.readlines():\n","            s, p, o = line.split()\n","            triples += [(s.strip(), p.strip(), o.strip())]\n","    return triples\n","\n","def load_triple():\n","    download()\n","    dataset_path = Config.data_path+Config.dataset_name+\"/\"\n","    triples_tr = read_triples(dataset_path+'train.txt')\n","    triples_va = read_triples(dataset_path+'valid.txt')\n","    triples_te = read_triples(dataset_path+'test.txt')\n","    triples_all = triples_tr + triples_va + triples_te\n","    return triples_all, triples_tr, triples_va, triples_te\n","\n","\n","def build_vocab(triples):\n","    params = {}\n","    e_set = {s for (s, p, o) in triples} | {o for (s, p, o) in triples}\n","    p_set = {p for (s, p, o) in triples}\n","    params['e_vocab_size'] = len(e_set)\n","    params['p_vocab_size'] = len(p_set)\n","    e2idx = {e: idx for idx, e in enumerate(sorted(e_set))}\n","    p2idx = {p: idx for idx, p in enumerate(sorted(p_set))}\n","    return e2idx, p2idx, params\n","\n","\n","def build_multi_label(triples_tr):\n","    sp2o = {}\n","    for (_s, _p, _o) in triples_tr:\n","        s, p, o = e2idx[_s], p2idx[_p], e2idx[_o] \n","        if (s,p) not in sp2o:\n","            sp2o[(s,p)] = [o]\n","        else:\n","            if o not in sp2o[(s,p)]:\n","                sp2o[(s,p)].append(o)\n","    return sp2o\n","  \n","def build_multi_label_all(triples_all):\n","    sp2o_all = {}\n","    for (_s, _p, _o) in triples_all:\n","        s, p, o = e2idx[_s], p2idx[_p], e2idx[_o] \n","        if (s,p) not in sp2o_all:\n","            sp2o_all[(s,p)] = [o]\n","        else:\n","            if o not in sp2o_all[(s,p)]:\n","                sp2o_all[(s,p)].append(o)\n","    return sp2o_all  \n","\n","\n","def get_train_y(triples_tr, e2idx, p2idx, sp2o):\n","    y = []\n","    for (_s, _p, _o) in triples_tr:\n","        s, p, o = e2idx[_s], p2idx[_p], e2idx[_o] \n","        temp = np.zeros([len(e2idx)])\n","        temp[sp2o[(s,p)]] = 1.\n","        y.append(temp)\n","    y = np.asarray(y)\n","    return y\n","\n","def get_eval_y(triples_te, e2idx, p2idx, sp2o_all):\n","    y = []\n","    for (_s, _p, _o) in triples_te:\n","        s, p, o = e2idx[_s], p2idx[_p], e2idx[_o] \n","        temp1 = np.zeros([len(e2idx)])\n","        temp1[o] = 1.\n","        temp2 = np.ones([len(e2idx)])\n","        temp2[sp2o_all[(s,p)]] = -1.\n","        temp2[o] = 1.\n","        y.append((temp1,temp2))\n","    y = np.asarray(y)\n","    return y \n","  \n","def get_features_labels(triples_tr, e2idx, p2idx, sp2o,entity_vocab_size):\n","  features = [[e2idx[s],p2idx[p]] for (s, p, o) in triples_tr]\n","  labels = [sp2o[(e2idx[s],p2idx[p])] for (s, p, o) in triples_tr]\n","  new_labels = np.full([len(labels),len(max(labels,key = lambda x: len(x)))],params['e_vocab_size']+1)\n","  for i,j in enumerate(labels):\n","    new_labels[i][0:len(j)] = j\n","  return features, new_labels"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nWM58omBXdzJ","colab_type":"code","colab":{}},"cell_type":"code","source":["def next_train_batch(triples_tr, e2idx, p2idx, sp2o):\n","    for i in range(0, len(triples_tr), Config.batch_size):\n","        _triples_tr = triples_tr[i: i+Config.batch_size]\n","        x_s = np.asarray([e2idx[s] for (s, p, o) in _triples_tr], dtype=np.int32)\n","        x_p = np.asarray([p2idx[p] for (s, p, o) in _triples_tr], dtype=np.int32)\n","        y = get_train_y(_triples_tr, e2idx, p2idx, sp2o)\n","        yield ({'s': x_s, 'p': x_p}, y)\n","\n","def train_input_fn(triples_tr, e2idx, p2idx, sp2o):\n","    dataset = tf.data.Dataset.from_generator(\n","        lambda: next_train_batch(triples_tr,\n","                                 e2idx,\n","                                 p2idx,\n","                                 sp2o),\n","        ({'s': tf.int32, 'p': tf.int32}, tf.float32),\n","        ({'s': tf.TensorShape([None]), 'p': tf.TensorShape([None])},\n","         tf.TensorShape([None, len(e2idx)])))\n","    dataset = dataset.shard(num_shards= NUM_SHARDS,  index = SHARD_INDEX)\n","    dataset = dataset.shuffle(buffer_size=Config.shuffle_buffer_size)\n","    dataset = dataset.repeat(Config.n_epochs)\n","    #dataset = dataset.batch(Config.batch_size)\n","    dataset = dataset.prefetch(Config.prefetch_buffer_size)\n","    return dataset\n","  \n","def next_train_single(triples_tr, e2idx, p2idx, sp2o):\n","    for i in range(0, len(triples_tr)):\n","        s,p,o = triples_tr[i]\n","        x_s = e2idx[s]\n","        x_p = p2idx[p] \n","        y = np.zeros([len(e2idx)])\n","        y[sp2o[(x_s,x_p)]] = 1.\n","        yield ({'s': x_s, 'p': x_p}, y)\n","        \n","def train_input_fn_single(triples_tr, e2idx, p2idx, sp2o):\n","    dataset = tf.data.Dataset.from_generator(\n","        lambda: next_train_single(triples_tr,\n","                                 e2idx,\n","                                 p2idx,\n","                                 sp2o),\n","        ({'s': tf.int32, 'p': tf.int32}, tf.float32),\n","        ({'s': tf.TensorShape([]), 'p': tf.TensorShape([])},\n","         tf.TensorShape([len(e2idx)])))\n","    #dataset = dataset.shard(num_shards= ,  index = )\n","    dataset = dataset.shuffle(buffer_size=Config.shuffle_buffer_size)\n","    dataset = dataset.repeat(Config.n_epochs)\n","    dataset = dataset.batch(Config.batch_size)\n","    dataset = dataset.prefetch(Config.prefetch_buffer_size)\n","    return dataset\n","\n","def next_eval_batch(triples_te, e2idx, p2idx, sp2o_all):\n","    for i in range(0, len(triples_te), Config.batch_size):\n","        _triples_te = triples_te[i: i+Config.batch_size]\n","        x_s = np.asarray([e2idx[s] for (s, p, o) in _triples_te], dtype=np.int32)\n","        x_p = np.asarray([p2idx[p] for (s, p, o) in _triples_te], dtype=np.int32)\n","        y = get_eval_y(_triples_te, e2idx, p2idx, sp2o_all)\n","        yield ({'s': x_s, 'p': x_p}, y)\n","  \n","def eval_input_fn(triples_te, e2idx, p2idx, sp2o_all):\n","    dataset = tf.data.Dataset.from_generator(\n","        lambda: next_eval_batch(triples_te,\n","                             e2idx, \n","                             p2idx,\n","                             sp2o_all),\n","        ({'s': tf.int32, 'p': tf.int32}, tf.float32),\n","        ({'s': tf.TensorShape([None]), 'p': tf.TensorShape([None])},\n","         tf.TensorShape([None, 2, len(e2idx)])))\n","    return dataset.prefetch(1)\n","  \n","def next_predict_batch(triples, e2idx, p2idx):\n","    for i in range(0, len(triples), Config.batch_size):\n","        _triples = triples[i: i+Config.batch_size]\n","        x_s = np.asarray([e2idx[s] for (s, p, o) in _triples], dtype=np.int32)\n","        x_p = np.asarray([p2idx[p] for (s, p, o) in _triples], dtype=np.int32)\n","        yield {'s': x_s, 'p': x_p}\n","  \n","def predict_input_fn(triples,\n","               e2idx, \n","               p2idx):\n","    dataset = tf.data.Dataset.from_generator(\n","        lambda: next_predict_batch(triples,\n","                             e2idx, \n","                             p2idx),\n","        ({'s':tf.int32, 'p':tf.int32}),\n","        ({'s':tf.TensorShape([None]),\n","         'p':tf.TensorShape([None])}))\n","    return dataset.prefetch(1)  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"2Q_yRnidgE_s","colab_type":"code","colab":{}},"cell_type":"code","source":[" def tf_get_rank_order(input, targets, filtered):\n","      target1 = targets[:,0]\n","      target2 = targets[:,1]\n","      tf.logging.info\n","      size = tf.shape(input)[-1]\n","      if filtered:\n","        filtered_input = tf.multiply(input,target2)\n","        indices_of_ranks = tf.nn.top_k(-filtered_input, k=size)[1]\n","      else:\n","        indices_of_ranks = tf.nn.top_k(-input, k=size)[1]\n","      indices_of_ranks = size - tf.nn.top_k(-indices_of_ranks, k=size)[1]\n","      indices_of_o = tf.cast(tf.argmax(target1, axis=1),tf.int32)\n","      row_indices = tf.range(tf.shape(indices_of_o)[0])\n","      full_indices = tf.stack([row_indices, indices_of_o], axis=1)\n","      return tf.gather_nd(indices_of_ranks, full_indices)\n","\n","def get_rank(logits, targets, filtered = False):\n","    ordered_array = tf_get_rank_order(logits, targets, filtered)\n","    return ordered_array"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0JmmEkJfXdzP","colab_type":"code","colab":{}},"cell_type":"code","source":["def forward(features, mode, params):\n","    e_embed = tf.get_variable('e_embed',\n","                              [params['e_vocab_size'], Config.embed_dim],\n","                              initializer=tf.contrib.layers.xavier_initializer())\n","    p_embed = tf.get_variable('p_embed',\n","                              [params['p_vocab_size'], Config.embed_dim],\n","                              initializer=tf.contrib.layers.xavier_initializer())\n","    \n","    s = tf.nn.embedding_lookup(e_embed, features['s'])\n","    p = tf.nn.embedding_lookup(p_embed, features['p'])\n","    \n","    logits = tf.matmul(s*p, e_embed, transpose_b=True)\n","    return logits  \n","    \n","def model_fn(features, labels, mode, params):\n","    logits = forward(features, mode, params)\n","    ps_strategy = params['ps_strategy']\n","    num_workers = params['num_workers']\n","    staleness = params['staleness']\n","    is_chief = params['is_chief']\n","    \n","    if mode == tf.estimator.ModeKeys.TRAIN:\n","        tf.logging.info('\\n'+pprint.pformat(tf.trainable_variables()))\n","        tf.logging.info('params: %d'%count_train_params())\n","        \n","        global_step = tf.train.get_global_step()\n","        \n","        loss_op = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,\n","                                                                        labels=labels))\n","        \n","        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n","          if ps_strategy == 'async':\n","            train_op = tf.train.AdamOptimizer().minimize(loss_op, global_step = global_step)\n","            sync_replicas_hook = None\n","          elif ps_strategy == 'sync':\n","            opt = tf.train.AdamOptimizer()\n","            opt = tf.train.SyncReplicasOptimizer(opt, replicas_to_aggregate=num_workers, total_num_replicas=num_workers)\n","            train_op = opt.minimize(loss_op, global_step = global_step)\n","            sync_replicas_hook = [opt.make_session_run_hook(is_chief)]\n","          elif ps_strategy == 'ssp':\n","            opt = tf.train.AdamOptimizer()\n","            opt = tf.contrib.opt.DropStaleGradientOptimizer(opt, staleness = staleness)\n","            train_op = opt.minimize(loss_op, global_step = global_step)\n","            sync_replicas_hook = None\n","        return tf.estimator.EstimatorSpec(mode = mode,\n","                                          loss = loss_op,\n","                                          train_op = train_op,\n","                                          training_hooks = sync_replicas_hook)\n","    \n","    if mode == tf.estimator.ModeKeys.PREDICT:\n","        return tf.estimator.EstimatorSpec(mode = mode, predictions = tf.sigmoid(logits))\n","      \n","    if mode == tf.estimator.ModeKeys.EVAL:\n","        filtered_rank = tf.cast(get_rank(tf.sigmoid(logits), labels, filtered = True), tf.float32)\n","        raw_rank = tf.cast(get_rank(tf.sigmoid(logits), labels, filtered = False), tf.float32)\n","        metrics = {\n","          'mean_rank_raw': tf.metrics.mean(raw_rank),\n","          'mean_rank_filtered': tf.metrics.mean(filtered_rank),\n","          'mean_reciprocal_rank_raw': tf.metrics.mean(1./raw_rank),\n","          'mean_reciprocal_rank_filtered': tf.metrics.mean(1./filtered_rank),\n","          'hits_at_1_raw' : tf.metrics.mean(raw_rank <= 1.),\n","          'hits_at_3_raw' : tf.metrics.mean(raw_rank <= 3.),\n","          'hits_at_5_raw' : tf.metrics.mean(raw_rank <= 5.),\n","          'hits_at_10_raw' : tf.metrics.mean(raw_rank <= 10.),\n","          'filtered_at_1_raw' : tf.metrics.mean(filtered_rank <= 1.),\n","          'filtered_at_3_raw' : tf.metrics.mean(filtered_rank <= 3.),\n","          'filtered_at_5_raw' : tf.metrics.mean(filtered_rank <= 5.),\n","          'filtered_at_10_raw' : tf.metrics.mean(filtered_rank <= 10.)\n","        }\n","        loss_op = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,\n","                                                                        labels=labels[:,0]))\n","        return tf.estimator.EstimatorSpec(mode = mode,loss=loss_op, eval_metric_ops=metrics)\n","\n","def count_train_params():\n","    return np.sum([np.prod([d.value for d in v.get_shape()]) for v in tf.trainable_variables()])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"960UHskCXdzU","colab_type":"code","outputId":"739bfaad-9e98-4f9b-9076-61b2a719a17e","executionInfo":{"status":"error","timestamp":1543927639830,"user_tz":-330,"elapsed":378,"user":{"displayName":"Nilesh Agrawal","photoUrl":"","userId":"15528777783423233741"}},"colab":{"base_uri":"https://localhost:8080/","height":357}},"cell_type":"code","source":["triples_all, triples_tr, triples_va, triples_te = load_triple()\n","e2idx, p2idx, params = build_vocab(triples_all)\n","sp2o = build_multi_label(triples_tr)\n","sp2o_all = build_multi_label_all(triples_all)\n","params['ps_strategy'] = PS_STRATEGY\n","params['num_workers'] = Config.num_workers\n","params['staleness'] = Config.staleness\n","params['is_chief'] = Config.is_chief\n","# features, labels = get_features_labels(triples_tr, e2idx, p2idx, sp2o,params['e_vocab_size'] )"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-4cfdf305ad55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtriples_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtriples_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtriples_va\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtriples_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_triple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0me2idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtriples_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msp2o\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_multi_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtriples_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msp2o_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_multi_label_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtriples_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ps_strategy'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPS_STRATEGY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-a2ad2bbb6221>\u001b[0m in \u001b[0;36mload_triple\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_triple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mdataset_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtriples_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_triples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'train.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'download' is not defined"]}]},{"metadata":{"id":"yHhaWtryLtEd","colab_type":"code","outputId":"48e73d31-0a07-4eec-d72a-c7423c9f0f99","executionInfo":{"status":"error","timestamp":1543927114837,"user_tz":-330,"elapsed":7928,"user":{"displayName":"Nilesh Agrawal","photoUrl":"","userId":"15528777783423233741"}},"colab":{"base_uri":"https://localhost:8080/","height":253}},"cell_type":"code","source":["strategy = tf.contrib.distribute.ParameterServerStrategy()\n","distribute_config = tf.contrib.distribute.DistributeConfig(train_distribute=strategy)\n","config = tf.estimator.RunConfig(model_dir=Config.model_dir,experimental_distribute=distribute_config)\n","model = tf.estimator.Estimator(model_fn,\n","                               params = params,\n","                              config = config)\n","training_time = time.time()\n","tf.logging.info('Model Training Started at Time: {}'.format(time.time()))\n","train_spec = tf.estimator.TrainSpec(lambda: train_input_fn(triples_tr, e2idx, p2idx, sp2o), max_steps = Config.n_epochs*Config.num_steps)\n","eval_spec = tf.estimator.EvalSpec(lambda: eval_input_fn(triples_te, e2idx, p2idx, sp2o_all))\n","#eval_spec = tf.estimator.EvalSpec(None)\n","tf.estimator.train_and_evaluate(model, train_spec, eval_spec)\n","# model.train(lambda: train_input_fn(triples_tr, e2idx, p2idx, sp2o))\n","tf.logging.info('Model Training Ended at Time: {}'.format(time.time()))\n","training_time = time.time() - training_time "],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:ParameterServerStrategy with compute_devices = ['/replica:0/task:0/device:CPU:0'], variable_device = '/device:CPU:0'\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-0d8a2167b72e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameterServerStrategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdistribute_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributeConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_distribute\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRunConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexperimental_distribute\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdistribute_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m model = tf.estimator.Estimator(model_fn,\n\u001b[1;32m      5\u001b[0m                                \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Config' is not defined"]}]},{"metadata":{"id":"963mztBdyvSn","colab_type":"code","colab":{}},"cell_type":"code","source":["tf.logging.info('Evaluation Started at Time: {}'.format(time.time()))\n","metrics = model.evaluate(lambda: eval_input_fn(triples_te, e2idx, p2idx, sp2o_all))\n","tf.logging.info('Evaluation Metrics: {}'.format(metrics))\n","tf.logging.info('Evaluation Loss:{}'.format(metrics['loss']))\n","tf.logging.info('Raw: Mean Rank: {}\\nRaw: Mean Reciprocal Rank: {}\\nRaw: Hits at 1: {}\\nRaw: Hits at 3: {}\\nRaw: Hits at 5: {}\\nRaw: Hits at 10: {}'.format(metrics['mean_rank_raw'],metrics['mean_reciprocal_rank_raw'],metrics['hits_at_1_raw'],metrics['hits_at_3_raw'],metrics['hits_at_5_raw'],metrics['hits_at_10_raw']))\n","tf.logging.info('Filtered: Mean Rank: {}\\nFiltered: Mean Reciprocal Rank: {}\\nFiltered: Hits at 1: {}\\nFiltered: Hits at 3: {}\\nFiltered: Hits at 5: {}\\nFiltered: Hits at 10: {}'.format(metrics['mean_rank_filtered'],metrics['mean_reciprocal_rank_filtered'],metrics['filtered_at_1_raw'],metrics['filtered_at_3_raw'],metrics['filtered_at_5_raw'],metrics['filtered_at_10_raw']))\n","print(metrics)\n","tf.logging.info('Evaluation Ended at Time: {}'.format(time.time()))\n","tf.logging.info('Training Time: {}'.format(training_time))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FbcOW6UJLQbB","colab_type":"code","colab":{}},"cell_type":"code","source":["'''def evaluate_rank(model, triples_va, triples_te, triples_all, e2idx, p2idx):\n","    for eval_name, eval_triples in [('test', triples_te)]:\n","        _scores_o = list(model.predict(lambda: predict_input_fn(eval_triples, e2idx, p2idx)))\n","        ScoresO = np.reshape(_scores_o, [len(eval_triples), len(e2idx)])\n","        ranks_o, filtered_ranks_o = [], []\n","        for ((s, p, o), scores_o) in tqdm(zip(eval_triples, ScoresO), total=len(eval_triples), ncols=70):\n","            s_idx, p_idx, o_idx = e2idx[s], p2idx[p], e2idx[o]\n","            ranks_o += [1 + np.argsort(np.argsort(- scores_o))[o_idx]]\n","            filtered_scores_o = scores_o.copy()\n","            rm_idx_o = [e2idx[fo] for (fs, fp, fo) in triples_all if fs == s and fp == p and fo != o]\n","            filtered_scores_o[rm_idx_o] = - np.inf\n","            filtered_ranks_o += [1 + np.argsort(np.argsort(- filtered_scores_o))[o_idx]]\n","        for setting_name, setting_ranks in [('Raw', ranks_o), ('Filtered', filtered_ranks_o)]:\n","            mean_rank = np.mean(np.asarray(setting_ranks))\n","            print('[{}] {} MR: {}'.format(eval_name, setting_name, mean_rank))\n","            mean_reciprocal_rank = np.mean(1 / np.asarray(setting_ranks))\n","            print('[{}] {} MRR: {}'.format(eval_name, setting_name, mean_reciprocal_rank))\n","            for k in [1, 3, 5, 10]:\n","                hits_at_k = np.mean(np.asarray(setting_ranks) <= k) * 100\n","                print('[{}] {} Hits@{}: {}'.format(eval_name, setting_name, k, hits_at_k))\n","                \n","evaluate_rank(model,triples_va, triples_te, triples_all, e2idx, p2idx,)'''"],"execution_count":0,"outputs":[]}]}