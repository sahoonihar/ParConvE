{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"literale_distributed_parameter_server.ipynb","version":"0.3.2","provenance":[{"file_id":"16fLW1Ck8uR22lx8un0ryT1OtW4EaP8S1","timestamp":1544202869084},{"file_id":"1Jc8rFThAR0qNAD9LBu1bsOj7HNt9E37r","timestamp":1543522742222}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"D7PbAsPQJajk","colab_type":"text"},"cell_type":"markdown","source":["1. Run this file: On GPU, using command \"python filename\".\n","2. Saved the model_dir and output testfor each run after training and testing\n","3. Change parameters num_gpu and use only FB15K for now.\n","4. Output will be four model_dir and output text one for each gpu spec 1/2/4/8."]},{"metadata":{"id":"Zh5bYbYMJVj5","colab_type":"code","outputId":"f15a1ca1-3dff-4a3f-d359-8a3fe7b3d38d","executionInfo":{"status":"ok","timestamp":1544208019206,"user_tz":-330,"elapsed":9401,"user":{"displayName":"Nilesh Agrawal","photoUrl":"","userId":"15528777783423233741"}},"colab":{"base_uri":"https://localhost:8080/","height":156}},"cell_type":"code","source":["from tqdm import tqdm\n","import tensorflow as tf\n","import numpy as np\n","import sklearn\n","import pprint\n","import itertools\n","import os\n","import time\n","import sys\n","import zipfile\n","#!pip install wget\n","import wget\n","import argparse\n","import json\n","import pandas\n","\n","sys.path.append(os.path.dirname(os.getcwd()))\n","tf.logging.set_verbosity(tf.logging.INFO)\n","# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting wget\n","  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n","Building wheels for collected packages: wget\n","  Running setup.py bdist_wheel for wget ... \u001b[?25l-\b \bdone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n","Successfully built wget\n","Installing collected packages: wget\n","Successfully installed wget-3.2\n"],"name":"stdout"}]},{"metadata":{"id":"VXqyae0HLyNd","colab_type":"code","colab":{}},"cell_type":"code","source":["ps = ['node15:3333']\n","chief = ['node16:3333']\n","worker = ['node17:3333','node18:3333','node19:3333' ]\n","cluster = {'chief' : chief,\n","            'ps' : ps,\n","             'worker' : worker}\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument(\"--type\", \"-t\", help=\"set node type (worker/ps/chief)\")\n","parser.add_argument(\"--index\", \"-i\", help=\"set node index\")\n","parser.add_argument(\"--strategy\", \"-s\", help=\"set strategy (sync/async/ssp)\")\n","args = parser.parse_args()\n","\n","os.environ['TF_CONFIG'] = json.dumps(\n","    {'cluster': cluster,\n","     'task': {'type': args.type, 'index': int(args.index)}})    \n","\n","NUM_SHARDS = len(chief)+len(worker)\n","SHARD_INDEX = int(args.index) if args.type == 'worker' else NUM_SHARDS -1\n","PS_STRATEGY = args.strategy"],"execution_count":0,"outputs":[]},{"metadata":{"id":"b_GYJiFa1C0I","colab_type":"code","colab":{}},"cell_type":"code","source":["class Config:\n","  n_epochs = 20\n","  batch_size = 128\n","  embed_dim = 200\n","  prefetch_buffer_size = None\n","  shuffle_buffer_size = 2\n","  map_threads = 3\n","  staleness = 30\n","  data_path = \"data/\"       #constant value\n","  dataset_name=\"FB15K\"      #FB15K/WN18/DB100K     #FB15K 272115, maxlen = 843         # DB100K 597572, maxlen = 85\n","  num_steps = 2120 if dataset_name == 'FB15K' else 4660\n","  num_workers = len(worker) + len(chief)\n","  num_ps = len(ps)\n","  is_chief = True if args.type == 'chief' else False\n","  model_dir = \"model/literale_\"+\"distributed_parameter_server\"+\"_strategy_\"+PS_STRATEGY+\"_ps_\"+str(num_ps)+\"_workers_\"+str(num_workers)+\"_bs_\"+str(batch_size)+\"_epochs_\"+str(n_epochs)+\"_embed_dim_\"+str(embed_dim)+\"_dataset_\"+dataset_name"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qsG1vbw2Xwhv","colab_type":"code","colab":{}},"cell_type":"code","source":["def download(data_path=Config.data_path, dataset_name=Config.dataset_name):\n","    dataset_path=data_path+dataset_name\n","    if not os.path.exists(dataset_path):\n","      os.makedirs(dataset_path)\n","    dataset_path = dataset_path+'/'\n","    if dataset_name==\"WN18\":\n","      if not os.path.isfile(dataset_path+'train.txt'):\n","          zip_path = dataset_path+\"WN18RR.zip\"\n","          url = \"https://www.dropbox.com/s/sginaquks2xzv6o/WN18RR.zip?dl=1\"\n","          wget.download(url, zip_path)\n","          z = zipfile.ZipFile(zip_path, 'r')\n","          z.extractall(dataset_path)\n","          z = z.close()\n","          os.remove(zip_path)\n","      else:\n","          print('Files Already Downloaded')\n","    elif dataset_name==\"FB15K\":\n","      if not os.path.isfile(dataset_path+'train.txt'):\n","          lit_path = dataset_path + '/numerical_literals.txt'\n","          url = 'https://www.dropbox.com/s/kc1woj05aetl58q/numerical_literals.txt?dl=1'\n","          wget.download(url, lit_path)\n","          zip_path = dataset_path+\"FB15K.zip\"\n","          url = \"https://www.dropbox.com/s/kph0mbs79w8itw6/FB15K-237.zip?dl=1\"\n","          wget.download(url, zip_path)\n","          z = zipfile.ZipFile(zip_path, 'r')\n","          z.extractall(dataset_path)\n","          z = z.close()\n","          os.remove(zip_path)\n","      else:\n","          print('Files Already Downloaded')\n","    elif dataset_name==\"DB100K\":\n","      if not os.path.isfile(dataset_path+'train.txt'):\n","        zip_path = dataset_path+\"DB100K.zip\"\n","        url = \"https://www.dropbox.com/s/fmbbh712ilx2zrc/DB100K.zip?dl=1\"\n","        wget.download(url, zip_path)\n","        z = zipfile.ZipFile(zip_path, 'r')\n","        z.extractall(dataset_path)\n","        z = z.close()\n","        os.remove(zip_path)\n","      else:\n","          print('Files Already Downloaded')\n","download()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"g7m3a1svXdzB","colab_type":"code","colab":{}},"cell_type":"code","source":["\"\"\"\n","e: entity\n","s: subject\n","p: predicate\n","o: object\n","\"\"\"\n","def glance_dict(d, n=5):\n","    return dict(itertools.islice(d.items(), n))\n","\n","def read_triples(path):\n","    triples = []\n","    with open(path, 'rt') as f:\n","        for line in f.readlines():\n","            s, p, o = line.split()\n","            triples += [(s.strip(), p.strip(), o.strip())]\n","    return triples\n","\n","def load_triple():\n","    download()\n","    dataset_path = Config.data_path+Config.dataset_name+\"/\"\n","    triples_tr = read_triples(dataset_path+'train.txt')\n","    triples_va = read_triples(dataset_path+'valid.txt')\n","    triples_te = read_triples(dataset_path+'test.txt')\n","    triples_all = triples_tr + triples_va + triples_te\n","    return triples_all, triples_tr, triples_va, triples_te\n","\n","\n","def build_vocab(triples):\n","    params = {}\n","    e_set = {s for (s, p, o) in triples} | {o for (s, p, o) in triples}\n","    p_set = {p for (s, p, o) in triples}\n","    params['e_vocab_size'] = len(e_set)\n","    params['p_vocab_size'] = len(p_set)\n","    e2idx = {e: idx for idx, e in enumerate(sorted(e_set))}\n","    p2idx = {p: idx for idx, p in enumerate(sorted(p_set))}\n","    return e2idx, p2idx, params\n","\n","\n","def build_multi_label(triples_tr):\n","    sp2o = {}\n","    for (_s, _p, _o) in triples_tr:\n","        s, p, o = e2idx[_s], p2idx[_p], e2idx[_o] \n","        if (s,p) not in sp2o:\n","            sp2o[(s,p)] = [o]\n","        else:\n","            if o not in sp2o[(s,p)]:\n","                sp2o[(s,p)].append(o)\n","    return sp2o\n","  \n","def build_multi_label_all(triples_all):\n","    sp2o_all = {}\n","    for (_s, _p, _o) in triples_all:\n","        s, p, o = e2idx[_s], p2idx[_p], e2idx[_o] \n","        if (s,p) not in sp2o_all:\n","            sp2o_all[(s,p)] = [o]\n","        else:\n","            if o not in sp2o_all[(s,p)]:\n","                sp2o_all[(s,p)].append(o)\n","    return sp2o_all  \n","\n","\n","def get_train_y(triples_tr, e2idx, p2idx, sp2o):\n","    y = []\n","    for (_s, _p, _o) in triples_tr:\n","        s, p, o = e2idx[_s], p2idx[_p], e2idx[_o] \n","        temp = np.zeros([len(e2idx)])\n","        temp[sp2o[(s,p)]] = 1.\n","        y.append(temp)\n","    y = np.asarray(y)\n","    return y\n","\n","def get_eval_y(triples_te, e2idx, p2idx, sp2o_all):\n","    y = []\n","    for (_s, _p, _o) in triples_te:\n","        s, p, o = e2idx[_s], p2idx[_p], e2idx[_o] \n","        temp1 = np.zeros([len(e2idx)])\n","        temp1[o] = 1.\n","        temp2 = np.ones([len(e2idx)])\n","        temp2[sp2o_all[(s,p)]] = -1.\n","        temp2[o] = 1.\n","        y.append((temp1,temp2))\n","    y = np.asarray(y)\n","    return y \n","  \n","def get_features_labels(triples_tr, e2idx, p2idx, sp2o,entity_vocab_size):\n","  features = [[e2idx[s],p2idx[p]] for (s, p, o) in triples_tr]\n","  labels = [sp2o[(e2idx[s],p2idx[p])] for (s, p, o) in triples_tr]\n","  new_labels = np.full([len(labels),len(max(labels,key = lambda x: len(x)))],params['e_vocab_size']+1)\n","  for i,j in enumerate(labels):\n","    new_labels[i][0:len(j)] = j\n","  return features, new_labels"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nWM58omBXdzJ","colab_type":"code","colab":{}},"cell_type":"code","source":["def next_train_batch(triples_tr, e2idx, p2idx, sp2o):\n","    for i in range(0, len(triples_tr), Config.batch_size):\n","        _triples_tr = triples_tr[i: i+Config.batch_size]\n","        x_s = np.asarray([e2idx[s] for (s, p, o) in _triples_tr], dtype=np.int32)\n","        x_p = np.asarray([p2idx[p] for (s, p, o) in _triples_tr], dtype=np.int32)\n","        y = get_train_y(_triples_tr, e2idx, p2idx, sp2o)\n","        yield ({'s': x_s, 'p': x_p}, y)\n","\n","def train_input_fn(triples_tr, e2idx, p2idx, sp2o):\n","    dataset = tf.data.Dataset.from_generator(\n","        lambda: next_train_batch(triples_tr,\n","                                 e2idx,\n","                                 p2idx,\n","                                 sp2o),\n","        ({'s': tf.int32, 'p': tf.int32}, tf.float32),\n","        ({'s': tf.TensorShape([None]), 'p': tf.TensorShape([None])},\n","         tf.TensorShape([None, len(e2idx)])))\n","    dataset = dataset.shard(num_shards= NUM_SHARDS,  index = SHARD_INDEX)\n","    dataset = dataset.shuffle(buffer_size=Config.shuffle_buffer_size)\n","    dataset = dataset.repeat(Config.n_epochs)\n","    #dataset = dataset.batch(Config.batch_size)\n","    dataset = dataset.prefetch(Config.prefetch_buffer_size)\n","    return dataset\n","  \n","def next_train_single(triples_tr, e2idx, p2idx, sp2o):\n","    for i in range(0, len(triples_tr)):\n","        s,p,o = triples_tr[i]\n","        x_s = e2idx[s]\n","        x_p = p2idx[p] \n","        y = np.zeros([len(e2idx)])\n","        y[sp2o[(x_s,x_p)]] = 1.\n","        yield ({'s': x_s, 'p': x_p}, y)\n","        \n","def train_input_fn_single(triples_tr, e2idx, p2idx, sp2o):\n","    dataset = tf.data.Dataset.from_generator(\n","        lambda: next_train_single(triples_tr,\n","                                 e2idx,\n","                                 p2idx,\n","                                 sp2o),\n","        ({'s': tf.int32, 'p': tf.int32}, tf.float32),\n","        ({'s': tf.TensorShape([]), 'p': tf.TensorShape([])},\n","         tf.TensorShape([len(e2idx)])))\n","    #dataset = dataset.shard(num_shards= ,  index = )\n","    dataset = dataset.shuffle(buffer_size=Config.shuffle_buffer_size)\n","    dataset = dataset.repeat(Config.n_epochs)\n","    dataset = dataset.batch(Config.batch_size)\n","    dataset = dataset.prefetch(Config.prefetch_buffer_size)\n","    return dataset\n","\n","def next_eval_batch(triples_te, e2idx, p2idx, sp2o_all):\n","    for i in range(0, len(triples_te), Config.batch_size):\n","        _triples_te = triples_te[i: i+Config.batch_size]\n","        x_s = np.asarray([e2idx[s] for (s, p, o) in _triples_te], dtype=np.int32)\n","        x_p = np.asarray([p2idx[p] for (s, p, o) in _triples_te], dtype=np.int32)\n","        y = get_eval_y(_triples_te, e2idx, p2idx, sp2o_all)\n","        yield ({'s': x_s, 'p': x_p}, y)\n","  \n","def eval_input_fn(triples_te, e2idx, p2idx, sp2o_all):\n","    dataset = tf.data.Dataset.from_generator(\n","        lambda: next_eval_batch(triples_te,\n","                             e2idx, \n","                             p2idx,\n","                             sp2o_all),\n","        ({'s': tf.int32, 'p': tf.int32}, tf.float32),\n","        ({'s': tf.TensorShape([None]), 'p': tf.TensorShape([None])},\n","         tf.TensorShape([None, 2, len(e2idx)])))\n","    return dataset.prefetch(1)\n","  \n","def next_predict_batch(triples, e2idx, p2idx):\n","    for i in range(0, len(triples), Config.batch_size):\n","        _triples = triples[i: i+Config.batch_size]\n","        x_s = np.asarray([e2idx[s] for (s, p, o) in _triples], dtype=np.int32)\n","        x_p = np.asarray([p2idx[p] for (s, p, o) in _triples], dtype=np.int32)\n","        yield {'s': x_s, 'p': x_p}\n","  \n","def predict_input_fn(triples,\n","               e2idx, \n","               p2idx):\n","    dataset = tf.data.Dataset.from_generator(\n","        lambda: next_predict_batch(triples,\n","                             e2idx, \n","                             p2idx),\n","        ({'s':tf.int32, 'p':tf.int32}),\n","        ({'s':tf.TensorShape([None]),\n","         'p':tf.TensorShape([None])}))\n","    return dataset.prefetch(1)  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"2Q_yRnidgE_s","colab_type":"code","colab":{}},"cell_type":"code","source":[" def tf_get_rank_order(input, targets, filtered):\n","      target1 = targets[:,0]\n","      target2 = targets[:,1]\n","      tf.logging.info\n","      size = tf.shape(input)[-1]\n","      if filtered:\n","        filtered_input = tf.multiply(input,target2)\n","        indices_of_ranks = tf.nn.top_k(-filtered_input, k=size)[1]\n","      else:\n","        indices_of_ranks = tf.nn.top_k(-input, k=size)[1]\n","      indices_of_ranks = size - tf.nn.top_k(-indices_of_ranks, k=size)[1]\n","      indices_of_o = tf.cast(tf.argmax(target1, axis=1),tf.int32)\n","      row_indices = tf.range(tf.shape(indices_of_o)[0])\n","      full_indices = tf.stack([row_indices, indices_of_o], axis=1)\n","      return tf.gather_nd(indices_of_ranks, full_indices)\n","\n","def get_rank(logits, targets, filtered = False):\n","    ordered_array = tf_get_rank_order(logits, targets, filtered)\n","    return ordered_array"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0JmmEkJfXdzP","colab_type":"code","colab":{}},"cell_type":"code","source":["def forward(features, mode, params):\n","    batch_sz = tf.shape(features['s'])[0]\n","    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n","    \n","    e_embed = tf.get_variable('e_embed',\n","                              [params['e_vocab_size'], Config.embed_dim],\n","                              initializer=tf.variance_scaling_initializer())\n","    p_embed = tf.get_variable('p_embed',\n","                              [params['p_vocab_size'], Config.embed_dim],\n","                              initializer=tf.variance_scaling_initializer())\n","    \n","    s = tf.nn.embedding_lookup(e_embed, features['s'])\n","    p = tf.nn.embedding_lookup(p_embed, features['p'])\n","    \n","    numerical_literals_embed = params['num_literals_embed']\n","    \n","    s_num_lit = tf.nn.embedding_lookup(numerical_literals_embed, features['s'])\n","    p_num_lit_size = tf.shape(numerical_literals)[1]\n","\n","    x = tf.concat([tf.reshape(s, [batch_sz, 200]),\n","                   tf.reshape(s_num_lit, [batch_sz, p_num_lit_size]),], axis=1)\n","    x = tf.layers.dense(x, Config.embed_dim, tf.nn.relu, name = 'fc_layer')\n","    \n","    x = tf.concat([tf.reshape(x, [batch_sz, 10, 20, 1]),\n","                   tf.reshape(p, [batch_sz, 10, 20, 1]),], axis=1)\n","    x = tf.layers.batch_normalization(x, training=is_training)\n","    x = tf.layers.dropout(x, 0.2, training=is_training)\n","    \n","    x = tf.layers.conv2d(x, 32, (3,3), activation=tf.nn.relu)\n","    x = tf.layers.flatten(x)\n","    x = tf.layers.batch_normalization(x, training=is_training)\n","    x = tf.layers.dropout(x, 0.2, training=is_training)\n","\n","    x = tf.layers.dense(x, Config.embed_dim, tf.nn.relu)\n","    x = tf.layers.batch_normalization(x, training=is_training)\n","    x = tf.layers.dropout(x, 0.3, training=is_training)\n","    \n","    new_e_embed = tf.layers.dense(tf.concat([e_embed,numerical_literals_embed],axis =1), Config.embed_dim, tf.nn.relu, name = 'fc_layer', reuse = True)                   \n","    logits = tf.matmul(x, new_e_embed, transpose_b=True)\n","    return logits\n","    \n","    \n","def model_fn(features, labels, mode, params):\n","    logits = forward(features, mode, params)\n","    ps_strategy = params['ps_strategy']\n","    num_workers = params['num_workers']\n","    staleness = params['staleness']\n","    is_chief = params['is_chief']\n","    \n","    if mode == tf.estimator.ModeKeys.TRAIN:\n","        tf.logging.info('\\n'+pprint.pformat(tf.trainable_variables()))\n","        tf.logging.info('params: %d'%count_train_params())\n","        \n","        global_step = tf.train.get_global_step()\n","        \n","        loss_op = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,\n","                                                                        labels=labels))\n","        \n","        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n","          if ps_strategy == 'async':\n","            train_op = tf.train.AdamOptimizer().minimize(loss_op, global_step = global_step)\n","            sync_replicas_hook = None\n","          elif ps_strategy == 'sync':\n","            opt = tf.train.AdamOptimizer()\n","            opt = tf.train.SyncReplicasOptimizer(opt, replicas_to_aggregate=num_workers, total_num_replicas=num_workers)\n","            train_op = opt.minimize(loss_op, global_step = global_step)\n","            sync_replicas_hook = [opt.make_session_run_hook(is_chief)]\n","          elif ps_strategy == 'ssp':\n","            opt = tf.train.AdamOptimizer()\n","            opt = tf.contrib.opt.DropStaleGradientOptimizer(opt, staleness = staleness)\n","            train_op = opt.minimize(loss_op, global_step = global_step)\n","            sync_replicas_hook = None\n","        return tf.estimator.EstimatorSpec(mode = mode,\n","                                          loss = loss_op,\n","                                          train_op = train_op,\n","                                          training_hooks = sync_replicas_hook)\n","    \n","    if mode == tf.estimator.ModeKeys.PREDICT:\n","        return tf.estimator.EstimatorSpec(mode = mode, predictions = tf.sigmoid(logits))\n","      \n","    if mode == tf.estimator.ModeKeys.EVAL:\n","        filtered_rank = tf.cast(get_rank(tf.sigmoid(logits), labels, filtered = True), tf.float32)\n","        raw_rank = tf.cast(get_rank(tf.sigmoid(logits), labels, filtered = False), tf.float32)\n","        metrics = {\n","          'mean_rank_raw': tf.metrics.mean(raw_rank),\n","          'mean_rank_filtered': tf.metrics.mean(filtered_rank),\n","          'mean_reciprocal_rank_raw': tf.metrics.mean(1./raw_rank),\n","          'mean_reciprocal_rank_filtered': tf.metrics.mean(1./filtered_rank),\n","          'hits_at_1_raw' : tf.metrics.mean(raw_rank <= 1.),\n","          'hits_at_3_raw' : tf.metrics.mean(raw_rank <= 3.),\n","          'hits_at_5_raw' : tf.metrics.mean(raw_rank <= 5.),\n","          'hits_at_10_raw' : tf.metrics.mean(raw_rank <= 10.),\n","          'filtered_at_1_raw' : tf.metrics.mean(filtered_rank <= 1.),\n","          'filtered_at_3_raw' : tf.metrics.mean(filtered_rank <= 3.),\n","          'filtered_at_5_raw' : tf.metrics.mean(filtered_rank <= 5.),\n","          'filtered_at_10_raw' : tf.metrics.mean(filtered_rank <= 10.)\n","        }\n","        loss_op = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,\n","                                                                        labels=labels[:,0]))\n","        return tf.estimator.EstimatorSpec(mode = mode,loss=loss_op, eval_metric_ops=metrics)\n","\n","def count_train_params():\n","    return np.sum([np.prod([d.value for d in v.get_shape()]) for v in tf.trainable_variables()])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"960UHskCXdzU","colab_type":"code","colab":{}},"cell_type":"code","source":["triples_all, triples_tr, triples_va, triples_te = load_triple()\n","e2idx, p2idx, params = build_vocab(triples_all)\n","sp2o = build_multi_label(triples_tr)\n","sp2o_all = build_multi_label_all(triples_all)\n","params['ps_strategy'] = PS_STRATEGY\n","params['num_workers'] = Config.num_workers\n","params['staleness'] = Config.staleness\n","params['is_chief'] = Config.is_chief\n","# features, labels = get_features_labels(triples_tr, e2idx, p2idx, sp2o,params['e_vocab_size'] )"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UiHqR4wZ2MKD","colab_type":"code","colab":{}},"cell_type":"code","source":["def build_lit_mat(e2idx):\n","    path = Config.data_path+Config.dataset_name+\"/\"\n","    df = pd.read_csv(path+'numerical_literals.txt', header=None, sep='\\t')\n","    rel2idx = {v: k for k, v in enumerate(df[1].unique())}\n","    num_lit = np.zeros([len(e2idx), len(rel2idx)], dtype=np.float32)\n","    # Create literal wrt vocab\n","    for i, (s, p, lit) in tqdm(enumerate(df.values)):\n","        try:\n","            num_lit[e2idx[s], rel2idx[p]] = lit\n","        except Exception :\n","            continue\n","    return num_lit\n","\n","numerical_literals = build_lit_mat(e2idx)\n","# Normalize literals\n","max_lit, min_lit = np.max(numerical_literals, axis=0), np.min(numerical_literals, axis=0)\n","numerical_literals = (numerical_literals - min_lit) / (max_lit - min_lit + 1e-8)\n","params['num_literals_embed'] = numerical_literals"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yHhaWtryLtEd","colab_type":"code","colab":{}},"cell_type":"code","source":["strategy = tf.contrib.distribute.ParameterServerStrategy()\n","distribute_config = tf.contrib.distribute.DistributeConfig(train_distribute=strategy)\n","config = tf.estimator.RunConfig(model_dir=Config.model_dir,experimental_distribute=distribute_config)\n","model = tf.estimator.Estimator(model_fn,\n","                               params = params,\n","                              config = config)\n","training_time = time.time()\n","tf.logging.info('Model Training Started at Time: {}'.format(time.time()))\n","train_spec = tf.estimator.TrainSpec(lambda: train_input_fn(triples_tr, e2idx, p2idx, sp2o), max_steps = Config.n_epochs*Config.num_steps)\n","eval_spec = tf.estimator.EvalSpec(lambda: eval_input_fn(triples_te, e2idx, p2idx, sp2o_all))\n","#eval_spec = tf.estimator.EvalSpec(None)\n","tf.estimator.train_and_evaluate(model, train_spec, eval_spec)\n","# model.train(lambda: train_input_fn(triples_tr, e2idx, p2idx, sp2o))\n","tf.logging.info('Model Training Ended at Time: {}'.format(time.time()))\n","training_time = time.time() - training_time "],"execution_count":0,"outputs":[]},{"metadata":{"id":"963mztBdyvSn","colab_type":"code","colab":{}},"cell_type":"code","source":["tf.logging.info('Evaluation Started at Time: {}'.format(time.time()))\n","metrics = model.evaluate(lambda: eval_input_fn(triples_te, e2idx, p2idx, sp2o_all))\n","tf.logging.info('Evaluation Metrics: {}'.format(metrics))\n","tf.logging.info('Evaluation Loss:{}'.format(metrics['loss']))\n","tf.logging.info('Raw: Mean Rank: {}\\nRaw: Mean Reciprocal Rank: {}\\nRaw: Hits at 1: {}\\nRaw: Hits at 3: {}\\nRaw: Hits at 5: {}\\nRaw: Hits at 10: {}'.format(metrics['mean_rank_raw'],metrics['mean_reciprocal_rank_raw'],metrics['hits_at_1_raw'],metrics['hits_at_3_raw'],metrics['hits_at_5_raw'],metrics['hits_at_10_raw']))\n","tf.logging.info('Filtered: Mean Rank: {}\\nFiltered: Mean Reciprocal Rank: {}\\nFiltered: Hits at 1: {}\\nFiltered: Hits at 3: {}\\nFiltered: Hits at 5: {}\\nFiltered: Hits at 10: {}'.format(metrics['mean_rank_filtered'],metrics['mean_reciprocal_rank_filtered'],metrics['filtered_at_1_raw'],metrics['filtered_at_3_raw'],metrics['filtered_at_5_raw'],metrics['filtered_at_10_raw']))\n","print(metrics)\n","tf.logging.info('Evaluation Ended at Time: {}'.format(time.time()))\n","tf.logging.info('Training Time: {}'.format(training_time))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FbcOW6UJLQbB","colab_type":"code","colab":{}},"cell_type":"code","source":["'''def evaluate_rank(model, triples_va, triples_te, triples_all, e2idx, p2idx):\n","    for eval_name, eval_triples in [('test', triples_te)]:\n","        _scores_o = list(model.predict(lambda: predict_input_fn(eval_triples, e2idx, p2idx)))\n","        ScoresO = np.reshape(_scores_o, [len(eval_triples), len(e2idx)])\n","        ranks_o, filtered_ranks_o = [], []\n","        for ((s, p, o), scores_o) in tqdm(zip(eval_triples, ScoresO), total=len(eval_triples), ncols=70):\n","            s_idx, p_idx, o_idx = e2idx[s], p2idx[p], e2idx[o]\n","            ranks_o += [1 + np.argsort(np.argsort(- scores_o))[o_idx]]\n","            filtered_scores_o = scores_o.copy()\n","            rm_idx_o = [e2idx[fo] for (fs, fp, fo) in triples_all if fs == s and fp == p and fo != o]\n","            filtered_scores_o[rm_idx_o] = - np.inf\n","            filtered_ranks_o += [1 + np.argsort(np.argsort(- filtered_scores_o))[o_idx]]\n","        for setting_name, setting_ranks in [('Raw', ranks_o), ('Filtered', filtered_ranks_o)]:\n","            mean_rank = np.mean(np.asarray(setting_ranks))\n","            print('[{}] {} MR: {}'.format(eval_name, setting_name, mean_rank))\n","            mean_reciprocal_rank = np.mean(1 / np.asarray(setting_ranks))\n","            print('[{}] {} MRR: {}'.format(eval_name, setting_name, mean_reciprocal_rank))\n","            for k in [1, 3, 5, 10]:\n","                hits_at_k = np.mean(np.asarray(setting_ranks) <= k) * 100\n","                print('[{}] {} Hits@{}: {}'.format(eval_name, setting_name, k, hits_at_k))\n","                \n","evaluate_rank(model,triples_va, triples_te, triples_all, e2idx, p2idx,)'''"],"execution_count":0,"outputs":[]}]}